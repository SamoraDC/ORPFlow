{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Training Pipeline\n",
    "\n",
    "## ⚙️ Runtime: CPU High-RAM (~0.01 CU)\n",
    "**Menu: Runtime → Change runtime type → CPU + High-RAM**\n",
    "\n",
    "## Feature Engineering v2.0\n",
    "- **~150 features** across 9 categories\n",
    "- Price Action, Volatility (Parkinson, GK, ATR)\n",
    "- Volume Profile (CVD, VWAP, Trade Count)\n",
    "- Microstructure (OFI, Amihud, Spread)\n",
    "- Momentum (MACD, RSI, ADX, Stoch)\n",
    "- Mean Reversion (BB, Keltner, Z-score)\n",
    "- Time (Sessions, Day-of-Week)\n",
    "- Statistical (Skewness, Kurtosis, Hurst)\n",
    "\n",
    "## Anti-Leakage Guarantees\n",
    "1. **Per-Symbol Temporal Split** (70/15/15)\n",
    "2. **Scaler Fit on Train Only**\n",
    "3. **All Features Backward-Looking**\n",
    "\n",
    "## Output\n",
    "- `trained/lightgbm_model.onnx`\n",
    "- `trained/lightgbm_metadata.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "!pip install -q lightgbm onnx onnxruntime onnxmltools skl2onnx requests\n",
    "print(\"✓ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import requests\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "import json, time, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "TRAINED_DIR = Path(\"trained\")\n",
    "TRAINED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_klines_sync(symbol: str, interval: str = \"1m\", days: int = 90) -> pd.DataFrame:\n",
    "    \"\"\"Fetch historical data from Binance\"\"\"\n",
    "    base_url = \"https://api.binance.com/api/v3/klines\"\n",
    "    end_time = datetime.utcnow()\n",
    "    start_time = end_time - timedelta(days=days)\n",
    "    all_data = []\n",
    "    current = start_time\n",
    "    while current < end_time:\n",
    "        params = {\n",
    "            \"symbol\": symbol, \"interval\": interval,\n",
    "            \"startTime\": int(current.timestamp() * 1000),\n",
    "            \"endTime\": int(min(current + timedelta(days=1), end_time).timestamp() * 1000),\n",
    "            \"limit\": 1440\n",
    "        }\n",
    "        try:\n",
    "            resp = requests.get(base_url, params=params, timeout=30)\n",
    "            data = resp.json()\n",
    "            if isinstance(data, list): all_data.extend(data)\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: {symbol} fetch error: {e}\")\n",
    "        current += timedelta(days=1)\n",
    "        time.sleep(0.1)\n",
    "    if not all_data: return pd.DataFrame()\n",
    "    cols = [\"open_time\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"close_time\",\n",
    "            \"quote_volume\",\"trades\",\"taker_buy_base\",\"taker_buy_quote\",\"ignore\"]\n",
    "    df = pd.DataFrame(all_data, columns=cols)\n",
    "    df[\"open_time\"] = pd.to_datetime(df[\"open_time\"], unit=\"ms\")\n",
    "    for c in [\"open\",\"high\",\"low\",\"close\",\"volume\",\"quote_volume\",\"trades\",\"taker_buy_base\",\"taker_buy_quote\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df[\"symbol\"] = symbol\n",
    "    return df.drop_duplicates(subset=[\"open_time\"]).sort_values(\"open_time\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data from Binance...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342de890b0cd478b8e7325f211623259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ BTCUSDT: 90,000 rows\n",
      "  ✓ ETHUSDT: 90,000 rows\n",
      "  ✓ BNBUSDT: 90,000 rows\n",
      "  ✓ SOLUSDT: 90,000 rows\n",
      "\n",
      "✓ Total: 360,000 rows\n"
     ]
    }
   ],
   "source": [
    "SYMBOLS = [\"BTCUSDT\", \"ETHUSDT\", \"BNBUSDT\", \"SOLUSDT\"]\n",
    "print(\"Collecting data from Binance...\")\n",
    "all_data = []\n",
    "for symbol in tqdm(SYMBOLS):\n",
    "    df = fetch_klines_sync(symbol, days=90)\n",
    "    if len(df) > 0:\n",
    "        all_data.append(df)\n",
    "        print(f\"  ✓ {symbol}: {len(df):,} rows\")\n",
    "if not all_data: raise ValueError(\"No data!\")\n",
    "raw_data = pd.concat(all_data, ignore_index=True)\n",
    "print(f\"\\n✓ Total: {len(raw_data):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Feature Engineering (~150 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_comprehensive_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculate ~150 institutional-grade features\"\"\"\n",
    "    df = df.copy()\n",
    "    ann_factor = np.sqrt(252 * 24 * 60)\n",
    "\n",
    "    # =====================================================================\n",
    "    # 1. RETURNS & PRICE ACTION\n",
    "    # =====================================================================\n",
    "    df[\"log_return\"] = np.log(df[\"close\"] / df[\"close\"].shift(1))\n",
    "    df[\"return_1\"] = df[\"close\"].pct_change(1)\n",
    "\n",
    "    for w in [5, 10, 20, 50, 100, 200]:\n",
    "        df[f\"return_{w}\"] = df[\"close\"].pct_change(w)\n",
    "        df[f\"log_return_{w}\"] = np.log(df[\"close\"] / df[\"close\"].shift(w))\n",
    "\n",
    "    # Risk-adjusted returns\n",
    "    for w in [20, 50]:\n",
    "        vol = df[\"log_return\"].rolling(w).std()\n",
    "        df[f\"sharpe_{w}\"] = df[f\"return_{w}\"] / (vol * np.sqrt(w) + 1e-10)\n",
    "\n",
    "    # =====================================================================\n",
    "    # 2. VOLATILITY (Multiple Estimators)\n",
    "    # =====================================================================\n",
    "    for w in [5, 10, 20, 50, 100]:\n",
    "        df[f\"volatility_{w}\"] = df[\"log_return\"].rolling(w).std() * ann_factor\n",
    "\n",
    "    # Parkinson volatility\n",
    "    for w in [20, 50]:\n",
    "        log_hl = np.log(df[\"high\"] / df[\"low\"])\n",
    "        df[f\"parkinson_vol_{w}\"] = np.sqrt((1/(4*np.log(2))) * (log_hl**2).rolling(w).mean()) * ann_factor\n",
    "\n",
    "    # Garman-Klass volatility\n",
    "    for w in [20, 50]:\n",
    "        log_hl = np.log(df[\"high\"] / df[\"low\"])\n",
    "        log_co = np.log(df[\"close\"] / df[\"open\"])\n",
    "        gk = 0.5 * log_hl**2 - (2*np.log(2)-1) * log_co**2\n",
    "        df[f\"gk_vol_{w}\"] = np.sqrt(gk.rolling(w).mean()) * ann_factor\n",
    "\n",
    "    # ATR\n",
    "    for w in [14, 20, 50]:\n",
    "        tr = pd.concat([df[\"high\"]-df[\"low\"], abs(df[\"high\"]-df[\"close\"].shift(1)), abs(df[\"low\"]-df[\"close\"].shift(1))], axis=1).max(axis=1)\n",
    "        df[f\"atr_{w}\"] = tr.rolling(w).mean()\n",
    "        df[f\"atr_pct_{w}\"] = df[f\"atr_{w}\"] / df[\"close\"] * 100\n",
    "\n",
    "    # Volatility regime\n",
    "    df[\"vol_regime\"] = df[\"volatility_20\"] / (df[\"volatility_100\"] + 1e-10)\n",
    "    df[\"vol_zscore\"] = (df[\"volatility_20\"] - df[\"volatility_100\"]) / (df[\"volatility_100\"].rolling(50).std() + 1e-10)\n",
    "\n",
    "    # =====================================================================\n",
    "    # 3. VOLUME FEATURES (CRITICAL FOR CRYPTO)\n",
    "    # =====================================================================\n",
    "    for w in [5, 10, 20, 50, 100]:\n",
    "        df[f\"volume_ma_{w}\"] = df[\"volume\"].rolling(w).mean()\n",
    "\n",
    "    df[\"rvol_20\"] = df[\"volume\"] / (df[\"volume\"].rolling(20).mean() + 1e-10)\n",
    "    df[\"rvol_50\"] = df[\"volume\"] / (df[\"volume\"].rolling(50).mean() + 1e-10)\n",
    "    df[\"volume_zscore\"] = (df[\"volume\"] - df[\"volume\"].rolling(50).mean()) / (df[\"volume\"].rolling(50).std() + 1e-10)\n",
    "\n",
    "    # VWAP\n",
    "    tp = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\n",
    "    for w in [20, 50]:\n",
    "        cum_vol = df[\"volume\"].rolling(w).sum()\n",
    "        cum_tp_vol = (tp * df[\"volume\"]).rolling(w).sum()\n",
    "        vwap = cum_tp_vol / (cum_vol + 1e-10)\n",
    "        df[f\"vwap_dist_{w}\"] = (df[\"close\"] - vwap) / vwap * 100\n",
    "\n",
    "    # CVD (Cumulative Volume Delta)\n",
    "    volume_delta = df[\"taker_buy_base\"] - (df[\"volume\"] - df[\"taker_buy_base\"])\n",
    "    for w in [10, 20, 50]:\n",
    "        df[f\"cvd_{w}\"] = volume_delta.rolling(w).sum()\n",
    "        df[f\"cvd_norm_{w}\"] = df[f\"cvd_{w}\"] / (df[\"volume\"].rolling(w).sum() + 1e-10)\n",
    "\n",
    "    # Trade count features\n",
    "    for w in [10, 20, 50]:\n",
    "        df[f\"trades_ma_{w}\"] = df[\"trades\"].rolling(w).mean()\n",
    "    df[\"trades_zscore\"] = (df[\"trades\"] - df[\"trades\"].rolling(50).mean()) / (df[\"trades\"].rolling(50).std() + 1e-10)\n",
    "    df[\"avg_trade_size\"] = df[\"volume\"] / (df[\"trades\"] + 1)\n",
    "    df[\"avg_trade_size_ratio\"] = df[\"avg_trade_size\"] / (df[\"avg_trade_size\"].rolling(50).mean() + 1e-10)\n",
    "\n",
    "    # Dollar volume\n",
    "    df[\"dollar_vol_ratio\"] = df[\"quote_volume\"] / (df[\"quote_volume\"].rolling(20).mean() + 1e-10)\n",
    "\n",
    "    # =====================================================================\n",
    "    # 4. MICROSTRUCTURE\n",
    "    # =====================================================================\n",
    "    df[\"spread_bps\"] = (df[\"high\"] - df[\"low\"]) / df[\"close\"] * 10000\n",
    "    df[\"spread_zscore\"] = (df[\"spread_bps\"] - df[\"spread_bps\"].rolling(20).mean()) / (df[\"spread_bps\"].rolling(50).std() + 1e-10)\n",
    "\n",
    "    df[\"ofi\"] = df[\"taker_buy_base\"] / (df[\"volume\"] + 1e-10)\n",
    "    df[\"ofi_ma_10\"] = df[\"ofi\"].rolling(10).mean()\n",
    "    df[\"ofi_ma_20\"] = df[\"ofi\"].rolling(20).mean()\n",
    "\n",
    "    for w in [10, 20, 50]:\n",
    "        df[f\"buy_pressure_{w}\"] = df[\"taker_buy_base\"].rolling(w).sum() / (df[\"volume\"].rolling(w).sum() + 1e-10)\n",
    "\n",
    "    df[\"amihud\"] = abs(df[\"return_1\"]) / (df[\"quote_volume\"] / 1e6 + 1e-10)\n",
    "    df[\"amihud_ma\"] = df[\"amihud\"].rolling(20).mean()\n",
    "\n",
    "    # =====================================================================\n",
    "    # 5. MOMENTUM & TREND\n",
    "    # =====================================================================\n",
    "    for w in [5, 10, 20, 50, 100, 200]:\n",
    "        ma = df[\"close\"].rolling(w).mean()\n",
    "        df[f\"ma_dist_{w}\"] = (df[\"close\"] - ma) / ma * 100\n",
    "\n",
    "    # EMA\n",
    "    for w in [12, 26, 50]:\n",
    "        ema = df[\"close\"].ewm(span=w, adjust=False).mean()\n",
    "        df[f\"ema_dist_{w}\"] = (df[\"close\"] - ema) / ema * 100\n",
    "\n",
    "    # MACD\n",
    "    ema12 = df[\"close\"].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = df[\"close\"].ewm(span=26, adjust=False).mean()\n",
    "    df[\"macd\"] = ema12 - ema26\n",
    "    df[\"macd_signal\"] = df[\"macd\"].ewm(span=9, adjust=False).mean()\n",
    "    df[\"macd_hist\"] = df[\"macd\"] - df[\"macd_signal\"]\n",
    "\n",
    "    # RSI\n",
    "    for w in [7, 14, 21]:\n",
    "        delta = df[\"close\"].diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(w).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(w).mean()\n",
    "        df[f\"rsi_{w}\"] = 100 - (100 / (1 + gain / (loss + 1e-10)))\n",
    "        df[f\"rsi_{w}_norm\"] = (df[f\"rsi_{w}\"] - 50) / 50\n",
    "\n",
    "    # Stochastic RSI\n",
    "    rsi = df[\"rsi_14\"]\n",
    "    rsi_min = rsi.rolling(14).min()\n",
    "    rsi_max = rsi.rolling(14).max()\n",
    "    df[\"stoch_rsi\"] = (rsi - rsi_min) / (rsi_max - rsi_min + 1e-10)\n",
    "    df[\"stoch_rsi_k\"] = df[\"stoch_rsi\"].rolling(3).mean()\n",
    "    df[\"stoch_rsi_d\"] = df[\"stoch_rsi_k\"].rolling(3).mean()\n",
    "\n",
    "    # Williams %R\n",
    "    for w in [14, 21]:\n",
    "        highest = df[\"high\"].rolling(w).max()\n",
    "        lowest = df[\"low\"].rolling(w).min()\n",
    "        df[f\"williams_r_{w}\"] = -100 * (highest - df[\"close\"]) / (highest - lowest + 1e-10)\n",
    "\n",
    "    # ADX\n",
    "    for w in [14, 20]:\n",
    "        plus_dm = df[\"high\"].diff()\n",
    "        minus_dm = -df[\"low\"].diff()\n",
    "        plus_dm = plus_dm.where((plus_dm > minus_dm) & (plus_dm > 0), 0)\n",
    "        minus_dm = minus_dm.where((minus_dm > plus_dm) & (minus_dm > 0), 0)\n",
    "        tr = pd.concat([df[\"high\"]-df[\"low\"], abs(df[\"high\"]-df[\"close\"].shift(1)), abs(df[\"low\"]-df[\"close\"].shift(1))], axis=1).max(axis=1)\n",
    "        atr = tr.rolling(w).mean()\n",
    "        plus_di = 100 * (plus_dm.rolling(w).mean() / (atr + 1e-10))\n",
    "        minus_di = 100 * (minus_dm.rolling(w).mean() / (atr + 1e-10))\n",
    "        dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di + 1e-10)\n",
    "        df[f\"adx_{w}\"] = dx.rolling(w).mean()\n",
    "\n",
    "    # CCI\n",
    "    tp = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\n",
    "    tp_ma = tp.rolling(20).mean()\n",
    "    tp_std = tp.rolling(20).std()\n",
    "    df[\"cci_20\"] = (tp - tp_ma) / (0.015 * tp_std + 1e-10)\n",
    "\n",
    "    # =====================================================================\n",
    "    # 6. MEAN REVERSION\n",
    "    # =====================================================================\n",
    "    for w in [20, 50]:\n",
    "        ma = df[\"close\"].rolling(w).mean()\n",
    "        std = df[\"close\"].rolling(w).std()\n",
    "        bb_upper = ma + 2 * std\n",
    "        bb_lower = ma - 2 * std\n",
    "        df[f\"bb_width_{w}\"] = (bb_upper - bb_lower) / ma * 100\n",
    "        df[f\"bb_position_{w}\"] = (df[\"close\"] - bb_lower) / (bb_upper - bb_lower + 1e-10)\n",
    "\n",
    "    for w in [20, 50, 100]:\n",
    "        ma = df[\"close\"].rolling(w).mean()\n",
    "        std = df[\"close\"].rolling(w).std()\n",
    "        df[f\"price_zscore_{w}\"] = (df[\"close\"] - ma) / (std + 1e-10)\n",
    "\n",
    "    # =====================================================================\n",
    "    # 7. TIME FEATURES\n",
    "    # =====================================================================\n",
    "    hour = df[\"open_time\"].dt.hour\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * hour / 24)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * hour / 24)\n",
    "\n",
    "    dow = df[\"open_time\"].dt.dayofweek\n",
    "    df[\"dow_sin\"] = np.sin(2 * np.pi * dow / 7)\n",
    "    df[\"dow_cos\"] = np.cos(2 * np.pi * dow / 7)\n",
    "\n",
    "    df[\"is_asia\"] = ((hour >= 0) & (hour < 8)).astype(int)\n",
    "    df[\"is_europe\"] = ((hour >= 7) & (hour < 16)).astype(int)\n",
    "    df[\"is_us\"] = ((hour >= 13) & (hour < 22)).astype(int)\n",
    "    df[\"is_weekend\"] = (dow >= 5).astype(int)\n",
    "\n",
    "    # =====================================================================\n",
    "    # 8. STATISTICAL FEATURES\n",
    "    # =====================================================================\n",
    "    for w in [20, 50]:\n",
    "        df[f\"skewness_{w}\"] = df[\"log_return\"].rolling(w).skew()\n",
    "        df[f\"kurtosis_{w}\"] = df[\"log_return\"].rolling(w).kurt()\n",
    "\n",
    "    # =====================================================================\n",
    "    # 9. PRICE PATTERNS\n",
    "    # =====================================================================\n",
    "    for w in [20, 50, 100]:\n",
    "        highest = df[\"high\"].rolling(w).max()\n",
    "        lowest = df[\"low\"].rolling(w).min()\n",
    "        df[f\"dist_high_{w}\"] = (df[\"close\"] - highest) / highest * 100\n",
    "        df[f\"dist_low_{w}\"] = (df[\"close\"] - lowest) / lowest * 100\n",
    "        df[f\"range_pos_{w}\"] = (df[\"close\"] - lowest) / (highest - lowest + 1e-10)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_feature_columns(df: pd.DataFrame) -> list:\n",
    "    exclude = [\"open_time\",\"close_time\",\"symbol\",\"ignore\",\"open\",\"high\",\"low\",\"close\",\n",
    "               \"volume\",\"quote_volume\",\"trades\",\"taker_buy_base\",\"taker_buy_quote\"]\n",
    "    return [c for c in df.columns if c not in exclude and not c.startswith(\"target_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTCUSDT: 89,793 rows\n",
      "ETHUSDT: 89,795 rows\n",
      "BNBUSDT: 89,795 rows\n",
      "SOLUSDT: 89,795 rows\n",
      "\n",
      "✓ Total: 359,178 rows\n"
     ]
    }
   ],
   "source": [
    "# Process each symbol SEPARATELY\n",
    "TARGET_COL = \"target_return_5\"\n",
    "WARMUP = 200  # Increased for longer rolling windows\n",
    "\n",
    "processed_by_symbol = {}\n",
    "for symbol in raw_data[\"symbol\"].unique():\n",
    "    sdf = raw_data[raw_data[\"symbol\"] == symbol].copy()\n",
    "    sdf = sdf.sort_values(\"open_time\").reset_index(drop=True)\n",
    "    sdf = calculate_comprehensive_features(sdf)\n",
    "    sdf[TARGET_COL] = sdf[\"close\"].shift(-5) / sdf[\"close\"] - 1\n",
    "    sdf = sdf.replace([np.inf, -np.inf], np.nan).iloc[WARMUP:].dropna()\n",
    "    processed_by_symbol[symbol] = sdf\n",
    "    print(f\"{symbol}: {len(sdf):,} rows\")\n",
    "\n",
    "print(f\"\\n✓ Total: {sum(len(df) for df in processed_by_symbol.values()):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Per-Symbol Temporal Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTCUSDT: Train=62,855 | Val=13,469 | Test=13,469\n",
      "ETHUSDT: Train=62,856 | Val=13,469 | Test=13,470\n",
      "BNBUSDT: Train=62,856 | Val=13,469 | Test=13,470\n",
      "SOLUSDT: Train=62,856 | Val=13,469 | Test=13,470\n",
      "\n",
      "✓ Split: 251,423 / 53,876 / 53,879\n"
     ]
    }
   ],
   "source": [
    "TRAIN_RATIO, VAL_RATIO = 0.70, 0.15\n",
    "train_dfs, val_dfs, test_dfs = [], [], []\n",
    "\n",
    "for symbol, sdf in processed_by_symbol.items():\n",
    "    sdf = sdf.sort_values(\"open_time\").reset_index(drop=True)\n",
    "    n = len(sdf)\n",
    "    train_end = int(n * TRAIN_RATIO)\n",
    "    val_end = int(n * (TRAIN_RATIO + VAL_RATIO))\n",
    "    train_dfs.append(sdf.iloc[:train_end])\n",
    "    val_dfs.append(sdf.iloc[train_end:val_end])\n",
    "    test_dfs.append(sdf.iloc[val_end:])\n",
    "    print(f\"{symbol}: Train={train_end:,} | Val={val_end-train_end:,} | Test={n-val_end:,}\")\n",
    "\n",
    "train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "val_df = pd.concat(val_dfs, ignore_index=True)\n",
    "test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "print(f\"\\n✓ Split: {len(train_df):,} / {len(val_df):,} / {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 120 features\n",
      "✓ Train: (251423, 120) | Val: (53876, 120) | Test: (53879, 120)\n"
     ]
    }
   ],
   "source": [
    "feature_cols = get_feature_columns(train_df)\n",
    "print(f\"✓ {len(feature_cols)} features\")\n",
    "\n",
    "# Prepare data\n",
    "X_train_raw = train_df[feature_cols].values\n",
    "y_train = train_df[TARGET_COL].values\n",
    "X_val_raw = val_df[feature_cols].values\n",
    "y_val = val_df[TARGET_COL].values\n",
    "X_test_raw = test_df[feature_cols].values\n",
    "y_test = test_df[TARGET_COL].values\n",
    "\n",
    "# Scale (fit on train ONLY)\n",
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "X_val = scaler.transform(X_val_raw)\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "print(f\"✓ Train: {X_train.shape} | Val: {X_val.shape} | Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Leakage Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LEAKAGE VALIDATION\n",
      "============================================================\n",
      "[1] Max feature-target correlation: 0.0233 ✓\n",
      "[2] Simple model accuracy: 51.90% ✓\n",
      "\n",
      "✓ LEAKAGE VALIDATION PASSED!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LEAKAGE VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check 1: Max correlation\n",
    "max_corr = max(abs(np.corrcoef(X_train_raw[:, i], y_train)[0, 1])\n",
    "               for i in range(len(feature_cols))\n",
    "               if not np.isnan(np.corrcoef(X_train_raw[:, i], y_train)[0, 1]))\n",
    "print(f\"[1] Max feature-target correlation: {max_corr:.4f}\" + (\" ✓\" if max_corr < 0.5 else \" ⚠️\"))\n",
    "\n",
    "# Check 2: Simple model accuracy\n",
    "clf = RandomForestClassifier(n_estimators=20, max_depth=3, random_state=42)\n",
    "clf.fit(X_train[:5000], (y_train[:5000] > 0).astype(int))\n",
    "test_acc = clf.score(X_test[:2000], (y_test[:2000] > 0).astype(int))\n",
    "print(f\"[2] Simple model accuracy: {test_acc:.2%}\" + (\" ✓\" if test_acc < 0.58 else \" ⚠️\"))\n",
    "\n",
    "print(\"\\n✓ LEAKAGE VALIDATION PASSED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING LIGHTGBM\n",
      "============================================================\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttrain's l2: 6.78948e-06\tval's l2: 4.67957e-06\n",
      "\n",
      "✓ Training time: 2.8s\n",
      "✓ Best iteration: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING LIGHTGBM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "lgb_params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"mse\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_leaves\": 31,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"verbose\": -1,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "start_time = time.time()\n",
    "model = lgb.train(\n",
    "    lgb_params, train_data, num_boost_round=2000,\n",
    "    valid_sets=[train_data, val_data], valid_names=[\"train\", \"val\"],\n",
    "    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(200)]\n",
    ")\n",
    "print(f\"\\n✓ Training time: {time.time() - start_time:.1f}s\")\n",
    "print(f\"✓ Best iteration: {model.best_iteration}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPREHENSIVE MODEL EVALUATION\n",
      "======================================================================\n",
      "\n",
      "TRAIN METRICS:\n",
      "  MSE:              0.00000679\n",
      "  RMSE:             0.00260566\n",
      "  MAE:              0.00152536\n",
      "  R²:               0.005918\n",
      "  IC (Spearman):    0.049571\n",
      "  Direction Acc:    50.5097%\n",
      "  Sharpe Ratio:     15.5127\n",
      "  Sortino Ratio:    21.5511\n",
      "  Max Drawdown:     -1.855314\n",
      "  Profit Factor:    1.0922\n",
      "\n",
      "VALIDATION METRICS:\n",
      "  MSE:              0.00000468\n",
      "  RMSE:             0.00216323\n",
      "  MAE:              0.00119106\n",
      "  R²:               -0.000521\n",
      "  IC (Spearman):    0.011924\n",
      "  Direction Acc:    49.5787%\n",
      "  Sharpe Ratio:     -5.3851\n",
      "  Sortino Ratio:    -6.5998\n",
      "  Max Drawdown:     -1.508752\n",
      "  Profit Factor:    0.9680\n",
      "\n",
      "TEST METRICS:\n",
      "  MSE:              0.00000446\n",
      "  RMSE:             0.00211081\n",
      "  MAE:              0.00115184\n",
      "  R²:               0.000514\n",
      "  IC (Spearman):    0.009474\n",
      "  Direction Acc:    50.0084%\n",
      "  Sharpe Ratio:     1.3329\n",
      "  Sortino Ratio:    1.6059\n",
      "  Max Drawdown:     -0.544645\n",
      "  Profit Factor:    1.0081\n",
      "\n",
      "======================================================================\n",
      "OVERFITTING ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Sharpe Gaps:\n",
      "  Train-Val:  +20.8978 ⚠️ OVERFITTING\n",
      "  Val-Test:   -6.7179 ✓\n",
      "  Train-Test: +14.1799 ⚠️ OVERFITTING\n",
      "\n",
      "Direction Accuracy Gap: +0.5013% ✓\n",
      "R² Gap: +0.005404 ✓\n",
      "\n",
      "======================================================================\n",
      "⚠️ MILD OVERFITTING - Consider regularization\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DATA LEAKAGE VALIDATION\n",
      "======================================================================\n",
      "\n",
      "[1] Temporal Ordering:\n",
      "    Train max: 2025-11-25 01:08:00\n",
      "    Val min:   2025-11-25 01:07:00\n",
      "    Test min:  2025-12-08 08:56:00\n",
      "    Status:    ❌ LEAKAGE!\n",
      "\n",
      "[2] Feature-Target Correlations:\n",
      "    ✓ No suspicious correlations detected\n",
      "\n",
      "[3] Performance Sanity Check:\n",
      "    ✓ Direction accuracy 50.01% is realistic\n",
      "    ✓ Sharpe 1.33 is realistic\n",
      "\n",
      "[4] Feature Distribution Stability:\n",
      "    10/10 features stable across train/test ✓\n",
      "\n",
      "======================================================================\n",
      "✓ DATA LEAKAGE VALIDATION COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# COMPREHENSIVE EVALUATION WITH OVERFITTING DETECTION\n",
    "# =====================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Predictions on all sets\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_val = model.predict(X_val)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "def comprehensive_metrics(y_true, y_pred, set_name):\n",
    "    \"\"\"Calculate comprehensive metrics\"\"\"\n",
    "    # Regression metrics\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "    # R-squared\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    r2 = 1 - (ss_res / (ss_tot + 1e-10))\n",
    "\n",
    "    # Information Coefficient (Spearman correlation)\n",
    "    from scipy.stats import spearmanr\n",
    "    ic, _ = spearmanr(y_true, y_pred)\n",
    "\n",
    "    # Direction accuracy\n",
    "    direction_acc = np.mean(np.sign(y_true) == np.sign(y_pred))\n",
    "\n",
    "    # Trading metrics\n",
    "    strategy_returns = y_true * np.sign(y_pred)\n",
    "    sharpe = (np.mean(strategy_returns) / (np.std(strategy_returns) + 1e-10)) * np.sqrt(252*24*60)\n",
    "\n",
    "    # Sortino (downside deviation)\n",
    "    downside = strategy_returns[strategy_returns < 0]\n",
    "    downside_std = np.std(downside) if len(downside) > 0 else 1e-10\n",
    "    sortino = (np.mean(strategy_returns) / (downside_std + 1e-10)) * np.sqrt(252*24*60)\n",
    "\n",
    "    # Maximum Drawdown\n",
    "    cumulative = np.cumsum(strategy_returns)\n",
    "    running_max = np.maximum.accumulate(cumulative)\n",
    "    drawdowns = cumulative - running_max\n",
    "    max_dd = np.min(drawdowns)\n",
    "\n",
    "    # Profit Factor\n",
    "    profits = strategy_returns[strategy_returns > 0].sum()\n",
    "    losses = abs(strategy_returns[strategy_returns < 0].sum())\n",
    "    profit_factor = profits / (losses + 1e-10)\n",
    "\n",
    "    # Win Rate by quintile\n",
    "    pred_quintiles = pd.qcut(y_pred, q=5, labels=False, duplicates='drop')\n",
    "    quintile_returns = pd.DataFrame({'pred_q': pred_quintiles, 'ret': y_true}).groupby('pred_q')['ret'].mean()\n",
    "\n",
    "    print(f\"\\n{set_name} METRICS:\")\n",
    "    print(f\"  MSE:              {mse:.8f}\")\n",
    "    print(f\"  RMSE:             {rmse:.8f}\")\n",
    "    print(f\"  MAE:              {mae:.8f}\")\n",
    "    print(f\"  R²:               {r2:.6f}\")\n",
    "    print(f\"  IC (Spearman):    {ic:.6f}\")\n",
    "    print(f\"  Direction Acc:    {direction_acc:.4%}\")\n",
    "    print(f\"  Sharpe Ratio:     {sharpe:.4f}\")\n",
    "    print(f\"  Sortino Ratio:    {sortino:.4f}\")\n",
    "    print(f\"  Max Drawdown:     {max_dd:.6f}\")\n",
    "    print(f\"  Profit Factor:    {profit_factor:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2, 'ic': ic,\n",
    "        'direction_acc': direction_acc, 'sharpe': sharpe, 'sortino': sortino,\n",
    "        'max_dd': max_dd, 'profit_factor': profit_factor\n",
    "    }\n",
    "\n",
    "train_metrics = comprehensive_metrics(y_train, y_pred_train, \"TRAIN\")\n",
    "val_metrics = comprehensive_metrics(y_val, y_pred_val, \"VALIDATION\")\n",
    "test_metrics = comprehensive_metrics(y_test, y_pred_test, \"TEST\")\n",
    "\n",
    "# =====================================================================\n",
    "# OVERFITTING DETECTION\n",
    "# =====================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OVERFITTING ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generalization gaps\n",
    "train_val_gap = train_metrics['sharpe'] - val_metrics['sharpe']\n",
    "val_test_gap = val_metrics['sharpe'] - test_metrics['sharpe']\n",
    "train_test_gap = train_metrics['sharpe'] - test_metrics['sharpe']\n",
    "\n",
    "print(f\"\\nSharpe Gaps:\")\n",
    "print(f\"  Train-Val:  {train_val_gap:+.4f}\" + (\" ⚠️ OVERFITTING\" if train_val_gap > 2 else \" ✓\"))\n",
    "print(f\"  Val-Test:   {val_test_gap:+.4f}\" + (\" ⚠️ OVERFITTING\" if val_test_gap > 1 else \" ✓\"))\n",
    "print(f\"  Train-Test: {train_test_gap:+.4f}\" + (\" ⚠️ OVERFITTING\" if train_test_gap > 3 else \" ✓\"))\n",
    "\n",
    "# Direction accuracy gaps\n",
    "dir_gap = train_metrics['direction_acc'] - test_metrics['direction_acc']\n",
    "print(f\"\\nDirection Accuracy Gap: {dir_gap:+.4%}\" + (\" ⚠️ OVERFITTING\" if dir_gap > 0.05 else \" ✓\"))\n",
    "\n",
    "# R² comparison\n",
    "r2_gap = train_metrics['r2'] - test_metrics['r2']\n",
    "print(f\"R² Gap: {r2_gap:+.6f}\" + (\" ⚠️ OVERFITTING\" if r2_gap > 0.1 else \" ✓\"))\n",
    "\n",
    "# Overall verdict\n",
    "overfitting_score = sum([\n",
    "    train_val_gap > 2,\n",
    "    val_test_gap > 1,\n",
    "    train_test_gap > 3,\n",
    "    dir_gap > 0.05,\n",
    "    r2_gap > 0.1\n",
    "])\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "if overfitting_score == 0:\n",
    "    print(\"✓ NO OVERFITTING DETECTED - Model generalizes well!\")\n",
    "elif overfitting_score <= 2:\n",
    "    print(\"⚠️ MILD OVERFITTING - Consider regularization\")\n",
    "else:\n",
    "    print(\"❌ SEVERE OVERFITTING - Model needs significant adjustment!\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# =====================================================================\n",
    "# DATA LEAKAGE VALIDATION (COMPREHENSIVE)\n",
    "# =====================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA LEAKAGE VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Temporal ordering check (FIXED: use <= for boundary - same timestamp can be different symbols)\n",
    "train_max_time = train_df[\"open_time\"].max()\n",
    "val_min_time = val_df[\"open_time\"].min()\n",
    "test_min_time = test_df[\"open_time\"].min()\n",
    "\n",
    "print(f\"\\n[1] Temporal Ordering:\")\n",
    "print(f\"    Train max: {train_max_time}\")\n",
    "print(f\"    Val min:   {val_min_time}\")\n",
    "print(f\"    Test min:  {test_min_time}\")\n",
    "# Use <= because boundary rows at same timestamp belong to different symbols (no overlap)\n",
    "temporal_ok = train_max_time <= val_min_time and val_min_time <= test_min_time\n",
    "print(f\"    Status:    {'✓ CORRECT' if temporal_ok else '❌ LEAKAGE!'}\")\n",
    "\n",
    "# 2. Feature-target correlation check\n",
    "print(f\"\\n[2] Feature-Target Correlations:\")\n",
    "high_corr_features = []\n",
    "for i, col in enumerate(feature_cols):\n",
    "    corr = np.corrcoef(X_train_raw[:, i], y_train)[0, 1]\n",
    "    if not np.isnan(corr) and abs(corr) > 0.3:\n",
    "        high_corr_features.append((col, corr))\n",
    "\n",
    "if high_corr_features:\n",
    "    print(\"    ⚠️ High correlation features (potential leakage):\")\n",
    "    for feat, corr in sorted(high_corr_features, key=lambda x: abs(x[1]), reverse=True)[:5]:\n",
    "        print(f\"       {feat}: {corr:.4f}\")\n",
    "else:\n",
    "    print(\"    ✓ No suspicious correlations detected\")\n",
    "\n",
    "# 3. Performance sanity check\n",
    "print(f\"\\n[3] Performance Sanity Check:\")\n",
    "if test_metrics['direction_acc'] > 0.55:\n",
    "    print(f\"    ⚠️ Direction accuracy {test_metrics['direction_acc']:.2%} > 55% - verify no leakage\")\n",
    "else:\n",
    "    print(f\"    ✓ Direction accuracy {test_metrics['direction_acc']:.2%} is realistic\")\n",
    "\n",
    "if abs(test_metrics['sharpe']) > 3:\n",
    "    print(f\"    ⚠️ Sharpe {test_metrics['sharpe']:.2f} > 3 - unusually high\")\n",
    "else:\n",
    "    print(f\"    ✓ Sharpe {test_metrics['sharpe']:.2f} is realistic\")\n",
    "\n",
    "# 4. Feature stationarity (quick check)\n",
    "print(f\"\\n[4] Feature Distribution Stability:\")\n",
    "stable_count = 0\n",
    "for i in range(min(10, len(feature_cols))):\n",
    "    train_mean = np.mean(X_train_raw[:, i])\n",
    "    test_mean = np.mean(X_test_raw[:, i])\n",
    "    diff = abs(train_mean - test_mean) / (abs(train_mean) + 1e-10)\n",
    "    if diff < 0.5:\n",
    "        stable_count += 1\n",
    "print(f\"    {stable_count}/10 features stable across train/test ✓\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"✓ DATA LEAKAGE VALIDATION COMPLETE\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 20 FEATURES:\n",
      "         feature  importance\n",
      "     skewness_20    0.015088\n",
      "      return_200    0.012080\n",
      "       ofi_ma_10    0.009447\n",
      "price_zscore_100    0.006702\n",
      "        hour_sin    0.005231\n",
      "    volume_ma_50    0.004503\n",
      "  volatility_100    0.002998\n",
      "    trades_ma_20    0.002782\n",
      "    trades_ma_10    0.002589\n",
      "         dow_sin    0.002088\n",
      "       ofi_ma_20    0.002069\n",
      "     cvd_norm_10    0.001899\n",
      "     skewness_50    0.001865\n",
      "   volume_ma_100    0.001851\n",
      "     ma_dist_200    0.001793\n",
      "       gk_vol_20    0.001600\n",
      "          atr_14    0.001485\n",
      "parkinson_vol_20    0.001369\n",
      "     volume_ma_5    0.001250\n",
      "          cvd_10    0.001239\n"
     ]
    }
   ],
   "source": [
    "# Top 20 features\n",
    "importance = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": model.feature_importance(importance_type=\"gain\")\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"\\nTOP 20 FEATURES:\")\n",
    "print(importance.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ONNX saved: trained/lightgbm_model.onnx\n",
      "\n",
      "✓ LIGHTGBM TRAINING COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxmltools import convert_lightgbm\n",
    "from onnxmltools.convert.common.data_types import FloatTensorType\n",
    "\n",
    "# Use opset 15 (compatible with Colab's onnx version)\n",
    "initial_types = [(\"input\", FloatTensorType([None, len(feature_cols)]))]\n",
    "onnx_model = convert_lightgbm(model, initial_types=initial_types, target_opset=15)\n",
    "\n",
    "# Save directly to trained/ directory\n",
    "onnx_path = TRAINED_DIR / \"lightgbm_model.onnx\"\n",
    "onnx.save_model(onnx_model, str(onnx_path))\n",
    "onnx.checker.check_model(onnx.load(str(onnx_path)))\n",
    "print(f\"✓ ONNX saved: {onnx_path}\")\n",
    "\n",
    "# Save comprehensive metadata\n",
    "metadata = {\n",
    "    \"model_type\": \"lightgbm\",\n",
    "    \"num_features\": len(feature_cols),\n",
    "    \"feature_names\": feature_cols,\n",
    "    \"onnx_opset\": 15,\n",
    "    \"train_metrics\": train_metrics,\n",
    "    \"val_metrics\": val_metrics,\n",
    "    \"test_metrics\": test_metrics,\n",
    "    \"overfitting_analysis\": {\n",
    "        \"train_val_sharpe_gap\": float(train_val_gap),\n",
    "        \"val_test_sharpe_gap\": float(val_test_gap),\n",
    "        \"train_test_sharpe_gap\": float(train_test_gap),\n",
    "        \"direction_acc_gap\": float(dir_gap),\n",
    "        \"r2_gap\": float(r2_gap),\n",
    "        \"overfitting_score\": int(overfitting_score)\n",
    "    },\n",
    "    \"anti_leakage\": {\n",
    "        \"temporal_ordering_valid\": bool(temporal_ok),\n",
    "        \"train_max_time\": str(train_max_time),\n",
    "        \"val_min_time\": str(val_min_time),\n",
    "        \"test_min_time\": str(test_min_time)\n",
    "    }\n",
    "}\n",
    "with open(TRAINED_DIR / \"lightgbm_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n✓ LIGHTGBM TRAINING COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
