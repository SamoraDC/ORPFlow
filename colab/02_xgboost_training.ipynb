{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# XGBoost Training Pipeline (GPU)\n\n## ⚙️ Runtime: T4 GPU (~0.28 CU)\n**Menu: Runtime → Change runtime type → T4 GPU**\n\n## Anti-Leakage Guarantees\n1. **Per-Symbol Temporal Split** - Each symbol split independently (70/15/15)\n2. **Scaler Fit on Train Only**\n3. **Backward-Looking Features**\n\n## Output\n- `trained/xgboost_model.onnx`\n- `trained/xgboost_metadata.json`"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 21 23:48:25 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.5/300.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.5/352.5 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h✓ Dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!pip install -q xgboost onnx onnxruntime-gpu onnxmltools requests\n",
    "print(\"✓ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport requests\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nfrom tqdm.notebook import tqdm\nimport json, time, warnings\nwarnings.filterwarnings('ignore')\n\nTRAINED_DIR = Path(\"trained\")\nTRAINED_DIR.mkdir(parents=True, exist_ok=True)\nprint(\"✓ Setup complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_klines_sync(symbol, days=90):\n",
    "    base_url = \"https://api.binance.com/api/v3/klines\"\n",
    "    end_time = datetime.utcnow()\n",
    "    start_time = end_time - timedelta(days=days)\n",
    "    all_data = []\n",
    "    current = start_time\n",
    "    while current < end_time:\n",
    "        params = {\"symbol\": symbol, \"interval\": \"1m\",\n",
    "                  \"startTime\": int(current.timestamp()*1000),\n",
    "                  \"endTime\": int(min(current+timedelta(days=1), end_time).timestamp()*1000), \"limit\": 1440}\n",
    "        try:\n",
    "            resp = requests.get(base_url, params=params, timeout=30)\n",
    "            data = resp.json()\n",
    "            if isinstance(data, list): all_data.extend(data)\n",
    "        except: pass\n",
    "        current += timedelta(days=1)\n",
    "        time.sleep(0.1)\n",
    "    if not all_data: return pd.DataFrame()\n",
    "    cols = [\"open_time\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"close_time\",\"quote_volume\",\"trades\",\"taker_buy_base\",\"taker_buy_quote\",\"ignore\"]\n",
    "    df = pd.DataFrame(all_data, columns=cols)\n",
    "    df[\"open_time\"] = pd.to_datetime(df[\"open_time\"], unit=\"ms\")\n",
    "    for c in [\"open\",\"high\",\"low\",\"close\",\"volume\",\"quote_volume\",\"taker_buy_base\",\"taker_buy_quote\",\"trades\"]: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df[\"symbol\"] = symbol\n",
    "    return df.drop_duplicates(subset=[\"open_time\"]).sort_values(\"open_time\").reset_index(drop=True)\n",
    "\n",
    "def calculate_comprehensive_features(df):\n",
    "    \"\"\"Calculate ~150 institutional-grade crypto features\"\"\"\n",
    "    df = df.copy()\n",
    "    ann_factor = np.sqrt(252 * 24 * 60)\n",
    "\n",
    "    # 1. RETURNS & PRICE ACTION\n",
    "    df[\"log_return\"] = np.log(df[\"close\"] / df[\"close\"].shift(1))\n",
    "    df[\"return_1\"] = df[\"close\"].pct_change(1)\n",
    "    for w in [5, 10, 20, 50, 100, 200]:\n",
    "        df[f\"return_{w}\"] = df[\"close\"].pct_change(w)\n",
    "    for w in [20, 50]:\n",
    "        vol = df[\"log_return\"].rolling(w).std()\n",
    "        df[f\"sharpe_{w}\"] = df[f\"return_{w}\"] / (vol * np.sqrt(w) + 1e-10)\n",
    "\n",
    "    # 2. VOLATILITY (multiple estimators)\n",
    "    for w in [5, 10, 20, 50, 100]:\n",
    "        df[f\"volatility_{w}\"] = df[\"log_return\"].rolling(w).std() * ann_factor\n",
    "    for w in [20, 50]:\n",
    "        log_hl = np.log(df[\"high\"] / df[\"low\"])\n",
    "        df[f\"parkinson_vol_{w}\"] = np.sqrt((1/(4*np.log(2))) * (log_hl**2).rolling(w).mean()) * ann_factor\n",
    "        log_co = np.log(df[\"close\"] / df[\"open\"])\n",
    "        gk = 0.5 * log_hl**2 - (2*np.log(2) - 1) * log_co**2\n",
    "        df[f\"gk_vol_{w}\"] = np.sqrt(gk.rolling(w).mean().abs()) * ann_factor\n",
    "    for w in [14, 20, 50]:\n",
    "        tr = pd.concat([df[\"high\"] - df[\"low\"], abs(df[\"high\"] - df[\"close\"].shift(1)), abs(df[\"low\"] - df[\"close\"].shift(1))], axis=1).max(axis=1)\n",
    "        df[f\"atr_{w}\"] = tr.rolling(w).mean()\n",
    "        df[f\"atr_pct_{w}\"] = df[f\"atr_{w}\"] / df[\"close\"] * 100\n",
    "    df[\"vol_regime\"] = df[\"volatility_20\"] / (df[\"volatility_100\"] + 1e-10)\n",
    "\n",
    "    # 3. VOLUME (CVD, VWAP, trades)\n",
    "    for w in [5, 10, 20, 50]:\n",
    "        df[f\"volume_ma_{w}\"] = df[\"volume\"].rolling(w).mean()\n",
    "    df[\"rvol_20\"] = df[\"volume\"] / (df[\"volume\"].rolling(20).mean() + 1e-10)\n",
    "    df[\"volume_zscore\"] = (df[\"volume\"] - df[\"volume\"].rolling(50).mean()) / (df[\"volume\"].rolling(50).std() + 1e-10)\n",
    "    typical_price = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\n",
    "    for w in [20, 50]:\n",
    "        cum_vol = df[\"volume\"].rolling(w).sum()\n",
    "        cum_tp_vol = (typical_price * df[\"volume\"]).rolling(w).sum()\n",
    "        df[f\"vwap_dist_{w}\"] = (df[\"close\"] - cum_tp_vol/(cum_vol+1e-10)) / (cum_tp_vol/(cum_vol+1e-10)+1e-10) * 100\n",
    "    volume_delta = df[\"taker_buy_base\"] - (df[\"volume\"] - df[\"taker_buy_base\"])\n",
    "    for w in [10, 20, 50]:\n",
    "        df[f\"cvd_{w}\"] = volume_delta.rolling(w).sum()\n",
    "        df[f\"cvd_norm_{w}\"] = df[f\"cvd_{w}\"] / (df[\"volume\"].rolling(w).sum() + 1e-10)\n",
    "    df[\"dollar_vol_ratio\"] = df[\"quote_volume\"] / (df[\"quote_volume\"].rolling(20).mean() + 1e-10)\n",
    "    if \"trades\" in df.columns:\n",
    "        df[\"trades\"] = pd.to_numeric(df[\"trades\"], errors=\"coerce\")\n",
    "        for w in [10, 20]:\n",
    "            df[f\"trades_ma_{w}\"] = df[\"trades\"].rolling(w).mean()\n",
    "        df[\"trades_zscore\"] = (df[\"trades\"] - df[\"trades\"].rolling(50).mean()) / (df[\"trades\"].rolling(50).std() + 1e-10)\n",
    "        df[\"avg_trade_size\"] = df[\"volume\"] / (df[\"trades\"] + 1)\n",
    "\n",
    "    # 4. MICROSTRUCTURE\n",
    "    df[\"spread_bps\"] = (df[\"high\"] - df[\"low\"]) / df[\"close\"] * 10000\n",
    "    df[\"ofi\"] = df[\"taker_buy_base\"] / (df[\"volume\"] + 1e-10)\n",
    "    for w in [10, 20, 50]:\n",
    "        df[f\"buy_pressure_{w}\"] = df[\"taker_buy_base\"].rolling(w).sum() / (df[\"volume\"].rolling(w).sum() + 1e-10)\n",
    "    df[\"amihud\"] = abs(df[\"return_1\"]) / (df[\"quote_volume\"] / 1e6 + 1e-10)\n",
    "\n",
    "    # 5. MOMENTUM (MACD, RSI, ADX, etc.)\n",
    "    for w in [5, 10, 20, 50, 100]:\n",
    "        df[f\"ma_dist_{w}\"] = (df[\"close\"] - df[\"close\"].rolling(w).mean()) / df[\"close\"].rolling(w).mean() * 100\n",
    "    ema12 = df[\"close\"].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = df[\"close\"].ewm(span=26, adjust=False).mean()\n",
    "    df[\"macd\"] = ema12 - ema26\n",
    "    df[\"macd_signal\"] = df[\"macd\"].ewm(span=9, adjust=False).mean()\n",
    "    df[\"macd_hist\"] = df[\"macd\"] - df[\"macd_signal\"]\n",
    "    for w in [7, 14, 21]:\n",
    "        delta = df[\"close\"].diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(w).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(w).mean()\n",
    "        df[f\"rsi_{w}\"] = 100 - (100 / (1 + gain/(loss+1e-10)))\n",
    "        df[f\"rsi_{w}_norm\"] = (df[f\"rsi_{w}\"] - 50) / 50\n",
    "    rsi14 = df[\"rsi_14\"]\n",
    "    rsi_min, rsi_max = rsi14.rolling(14).min(), rsi14.rolling(14).max()\n",
    "    df[\"stoch_rsi\"] = (rsi14 - rsi_min) / (rsi_max - rsi_min + 1e-10)\n",
    "    for w in [14, 21]:\n",
    "        highest, lowest = df[\"high\"].rolling(w).max(), df[\"low\"].rolling(w).min()\n",
    "        df[f\"williams_r_{w}\"] = -100 * (highest - df[\"close\"]) / (highest - lowest + 1e-10)\n",
    "    for w in [14, 20]:\n",
    "        plus_dm = df[\"high\"].diff().where(lambda x: x > 0, 0)\n",
    "        minus_dm = (-df[\"low\"].diff()).where(lambda x: x > 0, 0)\n",
    "        tr = pd.concat([df[\"high\"]-df[\"low\"], abs(df[\"high\"]-df[\"close\"].shift(1)), abs(df[\"low\"]-df[\"close\"].shift(1))], axis=1).max(axis=1)\n",
    "        atr = tr.rolling(w).mean()\n",
    "        plus_di = 100 * (plus_dm.rolling(w).mean() / (atr + 1e-10))\n",
    "        minus_di = 100 * (minus_dm.rolling(w).mean() / (atr + 1e-10))\n",
    "        df[f\"adx_{w}\"] = (100 * abs(plus_di - minus_di) / (plus_di + minus_di + 1e-10)).rolling(w).mean()\n",
    "    tp = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\n",
    "    df[\"cci_20\"] = (tp - tp.rolling(20).mean()) / (0.015 * tp.rolling(20).std() + 1e-10)\n",
    "\n",
    "    # 6. MEAN REVERSION (Bollinger, z-scores)\n",
    "    for w in [20, 50]:\n",
    "        ma, std = df[\"close\"].rolling(w).mean(), df[\"close\"].rolling(w).std()\n",
    "        df[f\"bb_width_{w}\"] = (4 * std) / ma * 100\n",
    "        df[f\"bb_position_{w}\"] = (df[\"close\"] - (ma - 2*std)) / (4*std + 1e-10)\n",
    "        df[f\"price_zscore_{w}\"] = (df[\"close\"] - ma) / (std + 1e-10)\n",
    "\n",
    "    # 7. TIME FEATURES\n",
    "    hour = df[\"open_time\"].dt.hour\n",
    "    dow = df[\"open_time\"].dt.dayofweek\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * hour / 24)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * hour / 24)\n",
    "    df[\"dow_sin\"] = np.sin(2 * np.pi * dow / 7)\n",
    "    df[\"dow_cos\"] = np.cos(2 * np.pi * dow / 7)\n",
    "    df[\"is_asia\"] = ((hour >= 0) & (hour < 8)).astype(int)\n",
    "    df[\"is_europe\"] = ((hour >= 7) & (hour < 16)).astype(int)\n",
    "    df[\"is_us\"] = ((hour >= 13) & (hour < 22)).astype(int)\n",
    "    df[\"is_weekend\"] = (dow >= 5).astype(int)\n",
    "\n",
    "    # 8. STATISTICAL\n",
    "    for w in [20, 50]:\n",
    "        df[f\"skewness_{w}\"] = df[\"log_return\"].rolling(w).skew()\n",
    "        df[f\"kurtosis_{w}\"] = df[\"log_return\"].rolling(w).kurt()\n",
    "\n",
    "    # 9. PRICE PATTERNS\n",
    "    for w in [20, 50, 100]:\n",
    "        highest, lowest = df[\"high\"].rolling(w).max(), df[\"low\"].rolling(w).min()\n",
    "        df[f\"dist_from_high_{w}\"] = (df[\"close\"] - highest) / highest * 100\n",
    "        df[f\"dist_from_low_{w}\"] = (df[\"close\"] - lowest) / lowest * 100\n",
    "        df[f\"range_position_{w}\"] = (df[\"close\"] - lowest) / (highest - lowest + 1e-10)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_feature_columns(df):\n",
    "    exclude = [\"open_time\",\"close_time\",\"symbol\",\"ignore\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"quote_volume\",\"trades\",\"taker_buy_base\",\"taker_buy_quote\"]\n",
    "    return [c for c in df.columns if c not in exclude and not c.startswith(\"target_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce899da794f84970acfee26f14e1dbaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ BTCUSDT: 90,000 rows\n",
      "  ✓ ETHUSDT: 90,000 rows\n",
      "  ✓ BNBUSDT: 90,000 rows\n",
      "  ✓ SOLUSDT: 90,000 rows\n",
      "\n",
      "✓ Total: 360,000 rows\n"
     ]
    }
   ],
   "source": [
    "SYMBOLS = [\"BTCUSDT\", \"ETHUSDT\", \"BNBUSDT\", \"SOLUSDT\"]\n",
    "print(\"Collecting data...\")\n",
    "all_data = []\n",
    "for sym in tqdm(SYMBOLS):\n",
    "    df = fetch_klines_sync(sym, days=90)\n",
    "    if len(df) > 0:\n",
    "        all_data.append(df)\n",
    "        print(f\"  ✓ {sym}: {len(df):,} rows\")\n",
    "    else:\n",
    "        print(f\"  ✗ {sym}: FAILED\")\n",
    "\n",
    "if not all_data: raise ValueError(\"No data!\")\n",
    "raw_data = pd.concat(all_data, ignore_index=True)\n",
    "print(f\"\\n✓ Total: {len(raw_data):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTCUSDT: 89,793 rows\n",
      "ETHUSDT: 89,795 rows\n",
      "BNBUSDT: 89,795 rows\n",
      "SOLUSDT: 89,795 rows\n",
      "\n",
      "✓ Split: 251,423/53,876/53,879\n",
      "✓ Features: 98\n"
     ]
    }
   ],
   "source": [
    "# Per-symbol split with comprehensive features\n",
    "TARGET_COL = \"target_return_5\"\n",
    "train_dfs, val_dfs, test_dfs = [], [], []\n",
    "\n",
    "for sym in raw_data[\"symbol\"].unique():\n",
    "    sdf = raw_data[raw_data[\"symbol\"]==sym].copy().sort_values(\"open_time\")\n",
    "    sdf = calculate_comprehensive_features(sdf)  # ~150 features\n",
    "    sdf[TARGET_COL] = sdf[\"close\"].shift(-5)/sdf[\"close\"] - 1\n",
    "    sdf = sdf.replace([np.inf,-np.inf], np.nan).iloc[200:].dropna()  # Extended warmup\n",
    "    n = len(sdf)\n",
    "    train_end, val_end = int(n*0.70), int(n*0.85)\n",
    "    train_dfs.append(sdf.iloc[:train_end])\n",
    "    val_dfs.append(sdf.iloc[train_end:val_end])\n",
    "    test_dfs.append(sdf.iloc[val_end:])\n",
    "    print(f\"{sym}: {len(sdf):,} rows\")\n",
    "\n",
    "train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "val_df = pd.concat(val_dfs, ignore_index=True)\n",
    "test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "print(f\"\\n✓ Split: {len(train_df):,}/{len(val_df):,}/{len(test_df):,}\")\n",
    "print(f\"✓ Features: {len(get_feature_columns(train_df))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 98 | Train: (251423, 98)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "feature_cols = get_feature_columns(train_df)\n",
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(train_df[feature_cols].values)\n",
    "X_val = scaler.transform(val_df[feature_cols].values)\n",
    "X_test = scaler.transform(test_df[feature_cols].values)\n",
    "y_train = train_df[TARGET_COL].values\n",
    "y_val = val_df[TARGET_COL].values\n",
    "y_test = test_df[TARGET_COL].values\n",
    "print(f\"Features: {len(feature_cols)} | Train: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING XGBOOST (GPU)\n",
      "============================================================\n",
      "[0]\ttrain-rmse:0.00260\tval-rmse:0.00209\n",
      "[100]\ttrain-rmse:0.00230\tval-rmse:0.00211\n",
      "\n",
      "✓ Training time: 1.3s\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING XGBOOST (GPU)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"device\": \"cuda\",\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "model = xgb.train(params, dtrain, num_boost_round=2000,\n",
    "                  evals=[(dtrain, \"train\"), (dval, \"val\")],\n",
    "                  early_stopping_rounds=100, verbose_eval=200)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"\\n✓ Training time: {train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =====================================================================\n# COMPREHENSIVE EVALUATION WITH OVERFITTING DETECTION\n# =====================================================================\nprint(\"=\" * 70)\nprint(\"COMPREHENSIVE MODEL EVALUATION\")\nprint(\"=\" * 70)\n\nfrom scipy.stats import spearmanr\n\n# Create DMatrix for test set (FIXED: was using undefined 'dtest')\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n# Predictions on all sets\ny_pred_train = model.predict(xgb.DMatrix(X_train))\ny_pred_val = model.predict(xgb.DMatrix(X_val))\ny_pred_test = model.predict(dtest)\n\ndef comprehensive_metrics(y_true, y_pred, set_name):\n    \"\"\"Calculate comprehensive metrics\"\"\"\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse = np.sqrt(mse)\n    mae = np.mean(np.abs(y_true - y_pred))\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    r2 = 1 - (ss_res / (ss_tot + 1e-10))\n    ic, _ = spearmanr(y_true, y_pred)\n    direction_acc = np.mean(np.sign(y_true) == np.sign(y_pred))\n    strategy_returns = y_true * np.sign(y_pred)\n    sharpe = (np.mean(strategy_returns) / (np.std(strategy_returns) + 1e-10)) * np.sqrt(252*24*60)\n    downside = strategy_returns[strategy_returns < 0]\n    downside_std = np.std(downside) if len(downside) > 0 else 1e-10\n    sortino = (np.mean(strategy_returns) / (downside_std + 1e-10)) * np.sqrt(252*24*60)\n    cumulative = np.cumsum(strategy_returns)\n    running_max = np.maximum.accumulate(cumulative)\n    max_dd = np.min(cumulative - running_max)\n    profits = strategy_returns[strategy_returns > 0].sum()\n    losses = abs(strategy_returns[strategy_returns < 0].sum())\n    profit_factor = profits / (losses + 1e-10)\n\n    print(f\"\\n{set_name} METRICS:\")\n    print(f\"  MSE: {mse:.8f} | RMSE: {rmse:.8f} | MAE: {mae:.8f}\")\n    print(f\"  R²: {r2:.6f} | IC: {ic:.6f} | Dir Acc: {direction_acc:.4%}\")\n    print(f\"  Sharpe: {sharpe:.4f} | Sortino: {sortino:.4f} | MaxDD: {max_dd:.6f}\")\n\n    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2, 'ic': ic,\n            'direction_acc': direction_acc, 'sharpe': sharpe, 'sortino': sortino,\n            'max_dd': max_dd, 'profit_factor': profit_factor}\n\ntrain_metrics = comprehensive_metrics(y_train, y_pred_train, \"TRAIN\")\nval_metrics = comprehensive_metrics(y_val, y_pred_val, \"VALIDATION\")\ntest_metrics = comprehensive_metrics(y_test, y_pred_test, \"TEST\")\n\n# OVERFITTING DETECTION\nprint(\"\\n\" + \"=\" * 70)\nprint(\"OVERFITTING ANALYSIS\")\nprint(\"=\" * 70)\n\ntrain_val_gap = train_metrics['sharpe'] - val_metrics['sharpe']\nval_test_gap = val_metrics['sharpe'] - test_metrics['sharpe']\ntrain_test_gap = train_metrics['sharpe'] - test_metrics['sharpe']\ndir_gap = train_metrics['direction_acc'] - test_metrics['direction_acc']\nr2_gap = train_metrics['r2'] - test_metrics['r2']\n\nprint(f\"\\nSharpe Gaps: Train-Val={train_val_gap:+.2f} | Val-Test={val_test_gap:+.2f} | Train-Test={train_test_gap:+.2f}\")\nprint(f\"Dir Acc Gap: {dir_gap:+.4%} | R² Gap: {r2_gap:+.6f}\")\n\noverfitting_score = sum([train_val_gap > 2, val_test_gap > 1, train_test_gap > 3, dir_gap > 0.05, r2_gap > 0.1])\nif overfitting_score == 0: print(\"\\n✓ NO OVERFITTING DETECTED\")\nelif overfitting_score <= 2: print(\"\\n⚠️ MILD OVERFITTING\")\nelse: print(\"\\n❌ SEVERE OVERFITTING\")\n\n# DATA LEAKAGE VALIDATION\nprint(\"\\n\" + \"=\" * 70)\nprint(\"DATA LEAKAGE VALIDATION\")\nprint(\"=\" * 70)\ntrain_max = train_df[\"open_time\"].max()\nval_min = val_df[\"open_time\"].min()\ntest_min = test_df[\"open_time\"].min()\n# Use <= for boundary check (rows at same timestamp are different symbols)\ntemporal_ok = train_max <= val_min and val_min <= test_min\nprint(f\"Temporal Order: Train<=Val<=Test = {'✓ CORRECT' if temporal_ok else '❌ LEAKAGE'}\")\nprint(f\"Dir Acc {test_metrics['direction_acc']:.2%}: {'✓ Realistic' if test_metrics['direction_acc'] < 0.55 else '⚠️ High'}\")\nprint(f\"Sharpe {test_metrics['sharpe']:.2f}: {'✓ Realistic' if abs(test_metrics['sharpe']) < 3 else '⚠️ High'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export ONNX with opset 15\nimport onnx\nfrom onnxmltools import convert_xgboost\nfrom onnxmltools.convert.common.data_types import FloatTensorType\n\ninitial_types = [(\"input\", FloatTensorType([None, len(feature_cols)]))]\nonnx_model = convert_xgboost(model, initial_types=initial_types, target_opset=15)\n\n# Save directly to trained/ directory\nonnx_path = TRAINED_DIR / \"xgboost_model.onnx\"\nonnx.save_model(onnx_model, str(onnx_path))\nonnx.checker.check_model(onnx.load(str(onnx_path)))\nprint(f\"✓ ONNX saved: {onnx_path}\")\n\nmetadata = {\n    \"model_type\": \"xgboost\", \"num_features\": len(feature_cols), \"onnx_opset\": 15,\n    \"train_metrics\": train_metrics, \"val_metrics\": val_metrics, \"test_metrics\": test_metrics,\n    \"overfitting_score\": int(overfitting_score),\n    \"temporal_ordering_valid\": bool(temporal_ok)\n}\nwith open(TRAINED_DIR / \"xgboost_metadata.json\", \"w\") as f:\n    json.dump(metadata, f, indent=2, default=str)\nprint(\"\\n✓ XGBOOST TRAINING COMPLETE!\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}