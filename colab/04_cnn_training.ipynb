{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Dilated CNN Training Pipeline\n\n## ⚙️ Runtime: T4 GPU (~0.56 CU)\n**Menu: Runtime → Change runtime type → T4 GPU**\n\n## Anti-Leakage Guarantees\n1. **Per-Symbol Temporal Split**\n2. **Sequences Created AFTER Split**\n3. **Scaler Fit on Train Only**\n\n## Output\n- `trained/cnn_model.onnx`\n- `trained/cnn_model.pt`\n- `trained/cnn_metadata.json`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\nimport torch\nprint(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\nif torch.cuda.is_available(): print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch onnx onnxruntime-gpu requests\nprint(\"✓ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nimport requests\nfrom sklearn.preprocessing import RobustScaler\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nfrom tqdm.notebook import tqdm\nimport json, time, warnings\nwarnings.filterwarnings('ignore')\n\nTRAINED_DIR = Path(\"trained\")\nTRAINED_DIR.mkdir(parents=True, exist_ok=True)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {DEVICE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def fetch_klines_sync(symbol, days=90):\n    base_url = \"https://api.binance.com/api/v3/klines\"\n    end_time = datetime.utcnow()\n    start_time = end_time - timedelta(days=days)\n    all_data = []\n    current = start_time\n    while current < end_time:\n        params = {\"symbol\": symbol, \"interval\": \"1m\",\n                  \"startTime\": int(current.timestamp()*1000),\n                  \"endTime\": int(min(current+timedelta(days=1), end_time).timestamp()*1000), \"limit\": 1440}\n        try:\n            resp = requests.get(base_url, params=params, timeout=30)\n            data = resp.json()\n            if isinstance(data, list): all_data.extend(data)\n        except: pass\n        current += timedelta(days=1)\n        time.sleep(0.1)\n    if not all_data: return pd.DataFrame()\n    cols = [\"open_time\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"close_time\",\"quote_volume\",\"trades\",\"taker_buy_base\",\"taker_buy_quote\",\"ignore\"]\n    df = pd.DataFrame(all_data, columns=cols)\n    df[\"open_time\"] = pd.to_datetime(df[\"open_time\"], unit=\"ms\")\n    for c in [\"open\",\"high\",\"low\",\"close\",\"volume\",\"quote_volume\",\"taker_buy_base\",\"taker_buy_quote\"]: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    df[\"symbol\"] = symbol\n    return df.drop_duplicates(subset=[\"open_time\"]).sort_values(\"open_time\")\n\ndef calculate_comprehensive_features(df):\n    \"\"\"Calculate ~150 institutional-grade crypto features\"\"\"\n    df = df.copy()\n    ann_factor = np.sqrt(252 * 24 * 60)\n\n    # 1. RETURNS & PRICE ACTION\n    df[\"log_return\"] = np.log(df[\"close\"] / df[\"close\"].shift(1))\n    df[\"return_1\"] = df[\"close\"].pct_change(1)\n    for w in [5, 10, 20, 50, 100, 200]:\n        df[f\"return_{w}\"] = df[\"close\"].pct_change(w)\n    for w in [20, 50]:\n        vol = df[\"log_return\"].rolling(w).std()\n        df[f\"sharpe_{w}\"] = df[f\"return_{w}\"] / (vol * np.sqrt(w) + 1e-10)\n\n    # 2. VOLATILITY (multiple estimators)\n    for w in [5, 10, 20, 50, 100]:\n        df[f\"volatility_{w}\"] = df[\"log_return\"].rolling(w).std() * ann_factor\n    for w in [20, 50]:\n        log_hl = np.log(df[\"high\"] / df[\"low\"])\n        df[f\"parkinson_vol_{w}\"] = np.sqrt((1/(4*np.log(2))) * (log_hl**2).rolling(w).mean()) * ann_factor\n        log_co = np.log(df[\"close\"] / df[\"open\"])\n        gk = 0.5 * log_hl**2 - (2*np.log(2) - 1) * log_co**2\n        df[f\"gk_vol_{w}\"] = np.sqrt(gk.rolling(w).mean().abs()) * ann_factor\n    for w in [14, 20, 50]:\n        tr = pd.concat([df[\"high\"] - df[\"low\"], abs(df[\"high\"] - df[\"close\"].shift(1)), abs(df[\"low\"] - df[\"close\"].shift(1))], axis=1).max(axis=1)\n        df[f\"atr_{w}\"] = tr.rolling(w).mean()\n        df[f\"atr_pct_{w}\"] = df[f\"atr_{w}\"] / df[\"close\"] * 100\n    df[\"vol_regime\"] = df[\"volatility_20\"] / (df[\"volatility_100\"] + 1e-10)\n\n    # 3. VOLUME (CVD, VWAP, trades)\n    for w in [5, 10, 20, 50]:\n        df[f\"volume_ma_{w}\"] = df[\"volume\"].rolling(w).mean()\n    df[\"rvol_20\"] = df[\"volume\"] / (df[\"volume\"].rolling(20).mean() + 1e-10)\n    df[\"volume_zscore\"] = (df[\"volume\"] - df[\"volume\"].rolling(50).mean()) / (df[\"volume\"].rolling(50).std() + 1e-10)\n    typical_price = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\n    for w in [20, 50]:\n        cum_vol = df[\"volume\"].rolling(w).sum()\n        cum_tp_vol = (typical_price * df[\"volume\"]).rolling(w).sum()\n        df[f\"vwap_dist_{w}\"] = (df[\"close\"] - cum_tp_vol/(cum_vol+1e-10)) / (cum_tp_vol/(cum_vol+1e-10)+1e-10) * 100\n    volume_delta = df[\"taker_buy_base\"] - (df[\"volume\"] - df[\"taker_buy_base\"])\n    for w in [10, 20, 50]:\n        df[f\"cvd_{w}\"] = volume_delta.rolling(w).sum()\n        df[f\"cvd_norm_{w}\"] = df[f\"cvd_{w}\"] / (df[\"volume\"].rolling(w).sum() + 1e-10)\n    df[\"dollar_vol_ratio\"] = df[\"quote_volume\"] / (df[\"quote_volume\"].rolling(20).mean() + 1e-10)\n\n    # 4. MICROSTRUCTURE\n    df[\"spread_bps\"] = (df[\"high\"] - df[\"low\"]) / df[\"close\"] * 10000\n    df[\"ofi\"] = df[\"taker_buy_base\"] / (df[\"volume\"] + 1e-10)\n    for w in [10, 20, 50]:\n        df[f\"buy_pressure_{w}\"] = df[\"taker_buy_base\"].rolling(w).sum() / (df[\"volume\"].rolling(w).sum() + 1e-10)\n    df[\"amihud\"] = abs(df[\"return_1\"]) / (df[\"quote_volume\"] / 1e6 + 1e-10)\n\n    # 5. MOMENTUM (MACD, RSI, ADX, etc.)\n    for w in [5, 10, 20, 50, 100]:\n        df[f\"ma_dist_{w}\"] = (df[\"close\"] - df[\"close\"].rolling(w).mean()) / df[\"close\"].rolling(w).mean() * 100\n    ema12 = df[\"close\"].ewm(span=12, adjust=False).mean()\n    ema26 = df[\"close\"].ewm(span=26, adjust=False).mean()\n    df[\"macd\"] = ema12 - ema26\n    df[\"macd_signal\"] = df[\"macd\"].ewm(span=9, adjust=False).mean()\n    df[\"macd_hist\"] = df[\"macd\"] - df[\"macd_signal\"]\n    for w in [7, 14, 21]:\n        delta = df[\"close\"].diff()\n        gain = delta.where(delta > 0, 0).rolling(w).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(w).mean()\n        df[f\"rsi_{w}\"] = 100 - (100 / (1 + gain/(loss+1e-10)))\n        df[f\"rsi_{w}_norm\"] = (df[f\"rsi_{w}\"] - 50) / 50\n    rsi14 = df[\"rsi_14\"]\n    rsi_min, rsi_max = rsi14.rolling(14).min(), rsi14.rolling(14).max()\n    df[\"stoch_rsi\"] = (rsi14 - rsi_min) / (rsi_max - rsi_min + 1e-10)\n    for w in [14, 21]:\n        highest, lowest = df[\"high\"].rolling(w).max(), df[\"low\"].rolling(w).min()\n        df[f\"williams_r_{w}\"] = -100 * (highest - df[\"close\"]) / (highest - lowest + 1e-10)\n    for w in [14, 20]:\n        plus_dm = df[\"high\"].diff().where(lambda x: x > 0, 0)\n        minus_dm = (-df[\"low\"].diff()).where(lambda x: x > 0, 0)\n        tr = pd.concat([df[\"high\"]-df[\"low\"], abs(df[\"high\"]-df[\"close\"].shift(1)), abs(df[\"low\"]-df[\"close\"].shift(1))], axis=1).max(axis=1)\n        atr = tr.rolling(w).mean()\n        plus_di = 100 * (plus_dm.rolling(w).mean() / (atr + 1e-10))\n        minus_di = 100 * (minus_dm.rolling(w).mean() / (atr + 1e-10))\n        df[f\"adx_{w}\"] = (100 * abs(plus_di - minus_di) / (plus_di + minus_di + 1e-10)).rolling(w).mean()\n    tp = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\n    df[\"cci_20\"] = (tp - tp.rolling(20).mean()) / (0.015 * tp.rolling(20).std() + 1e-10)\n\n    # 6. MEAN REVERSION (Bollinger, z-scores)\n    for w in [20, 50]:\n        ma, std = df[\"close\"].rolling(w).mean(), df[\"close\"].rolling(w).std()\n        df[f\"bb_width_{w}\"] = (4 * std) / ma * 100\n        df[f\"bb_position_{w}\"] = (df[\"close\"] - (ma - 2*std)) / (4*std + 1e-10)\n        df[f\"price_zscore_{w}\"] = (df[\"close\"] - ma) / (std + 1e-10)\n\n    # 7. TIME FEATURES\n    hour = df[\"open_time\"].dt.hour\n    dow = df[\"open_time\"].dt.dayofweek\n    df[\"hour_sin\"] = np.sin(2 * np.pi * hour / 24)\n    df[\"hour_cos\"] = np.cos(2 * np.pi * hour / 24)\n    df[\"dow_sin\"] = np.sin(2 * np.pi * dow / 7)\n    df[\"dow_cos\"] = np.cos(2 * np.pi * dow / 7)\n    df[\"is_asia\"] = ((hour >= 0) & (hour < 8)).astype(int)\n    df[\"is_europe\"] = ((hour >= 7) & (hour < 16)).astype(int)\n    df[\"is_us\"] = ((hour >= 13) & (hour < 22)).astype(int)\n    df[\"is_weekend\"] = (dow >= 5).astype(int)\n\n    # 8. STATISTICAL\n    for w in [20, 50]:\n        df[f\"skewness_{w}\"] = df[\"log_return\"].rolling(w).skew()\n        df[f\"kurtosis_{w}\"] = df[\"log_return\"].rolling(w).kurt()\n\n    # 9. PRICE PATTERNS\n    for w in [20, 50, 100]:\n        highest, lowest = df[\"high\"].rolling(w).max(), df[\"low\"].rolling(w).min()\n        df[f\"dist_from_high_{w}\"] = (df[\"close\"] - highest) / highest * 100\n        df[f\"dist_from_low_{w}\"] = (df[\"close\"] - lowest) / lowest * 100\n        df[f\"range_position_{w}\"] = (df[\"close\"] - lowest) / (highest - lowest + 1e-10)\n\n    return df\n\ndef get_feature_columns(df):\n    exclude = [\"open_time\",\"close_time\",\"symbol\",\"ignore\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"quote_volume\",\"trades\",\"taker_buy_base\",\"taker_buy_quote\"]\n    return [c for c in df.columns if c not in exclude and not c.startswith(\"target_\")]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMBOLS = [\"BTCUSDT\", \"ETHUSDT\", \"BNBUSDT\", \"SOLUSDT\"]\nprint(\"Collecting data...\")\nall_data = []\nfor sym in tqdm(SYMBOLS):\n    df = fetch_klines_sync(sym, days=90)\n    if len(df) > 0:\n        all_data.append(df)\n        print(f\"  ✓ {sym}: {len(df):,} rows\")\nraw_data = pd.concat(all_data, ignore_index=True)\nprint(f\"\\n✓ Total: {len(raw_data):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Per-symbol split with comprehensive features\nTARGET_COL = \"target_return_5\"\nSEQUENCE_LENGTH = 60\ntrain_dfs, val_dfs, test_dfs = [], [], []\n\nfor sym in raw_data[\"symbol\"].unique():\n    sdf = raw_data[raw_data[\"symbol\"]==sym].copy().sort_values(\"open_time\").reset_index(drop=True)\n    sdf = calculate_comprehensive_features(sdf)  # ~150 features\n    sdf[TARGET_COL] = sdf[\"close\"].shift(-5)/sdf[\"close\"] - 1\n    sdf = sdf.replace([np.inf,-np.inf], np.nan).iloc[200:].dropna()  # Extended warmup\n    n = len(sdf)\n    train_end, val_end = int(n*0.70), int(n*0.85)\n    train_dfs.append(sdf.iloc[:train_end])\n    val_dfs.append(sdf.iloc[train_end:val_end])\n    test_dfs.append(sdf.iloc[val_end:])\n\ntrain_df = pd.concat(train_dfs).sort_values(\"open_time\").reset_index(drop=True)\nval_df = pd.concat(val_dfs).sort_values(\"open_time\").reset_index(drop=True)\ntest_df = pd.concat(test_dfs).sort_values(\"open_time\").reset_index(drop=True)\nprint(f\"✓ Split: {len(train_df):,}/{len(val_df):,}/{len(test_df):,}\")\nprint(f\"✓ Features: {len(get_feature_columns(train_df))}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences\nfeature_cols = get_feature_columns(train_df)\nscaler = RobustScaler()\nX_train_scaled = scaler.fit_transform(train_df[feature_cols].values)\nX_val_scaled = scaler.transform(val_df[feature_cols].values)\nX_test_scaled = scaler.transform(test_df[feature_cols].values)\n\ndef create_sequences(X, y, seq_len):\n    X_seq, y_seq = [], []\n    for i in range(seq_len, len(X)):\n        X_seq.append(X[i-seq_len:i])\n        y_seq.append(y[i])\n    return np.array(X_seq), np.array(y_seq)\n\nX_train_seq, y_train_seq = create_sequences(X_train_scaled, train_df[TARGET_COL].values, SEQUENCE_LENGTH)\nX_val_seq, y_val_seq = create_sequences(X_val_scaled, val_df[TARGET_COL].values, SEQUENCE_LENGTH)\nX_test_seq, y_test_seq = create_sequences(X_test_scaled, test_df[TARGET_COL].values, SEQUENCE_LENGTH)\nprint(f\"Sequences: {X_train_seq.shape}/{X_val_seq.shape}/{X_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNetwork(nn.Module):\n    def __init__(self, num_features, channels=[64, 128, 256], dropout=0.3):\n        super().__init__()\n        self.input_proj = nn.Conv1d(num_features, channels[0], kernel_size=1)\n        self.conv_blocks = nn.ModuleList()\n        in_ch = channels[0]\n        for i, out_ch in enumerate(channels):\n            self.conv_blocks.append(nn.Sequential(\n                nn.Conv1d(in_ch, out_ch, kernel_size=3, padding=1, dilation=2**i),\n                nn.BatchNorm1d(out_ch), nn.ReLU(), nn.Dropout(dropout)\n            ))\n            in_ch = out_ch\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc = nn.Sequential(nn.Linear(channels[-1], 256), nn.ReLU(), nn.Dropout(dropout), nn.Linear(256, 1))\n\n    def forward(self, x):\n        x = x.transpose(1, 2)\n        x = self.input_proj(x)\n        for block in self.conv_blocks: x = block(x)\n        x = self.global_pool(x).squeeze(-1)\n        return self.fc(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\nprint(\"TRAINING CNN (CUDA)\")\nprint(\"=\" * 60)\n\nnum_features = X_train_seq.shape[2]\nmodel = CNNNetwork(num_features).to(DEVICE)\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\ncriterion = nn.MSELoss()\n\ntrain_loader = DataLoader(TensorDataset(torch.FloatTensor(X_train_seq), torch.FloatTensor(y_train_seq)), batch_size=256, shuffle=True)\nval_loader = DataLoader(TensorDataset(torch.FloatTensor(X_val_seq), torch.FloatTensor(y_val_seq)), batch_size=256)\n\nbest_val_loss, patience_counter = float('inf'), 0\nstart_time = time.time()\n\nfor epoch in range(100):\n    model.train()\n    train_loss = sum(criterion(model(X.to(DEVICE)), y.to(DEVICE)).item() for X, y in train_loader) / len(train_loader)\n    \n    model.eval()\n    with torch.no_grad():\n        val_loss = sum(criterion(model(X.to(DEVICE)), y.to(DEVICE)).item() for X, y in val_loader) / len(val_loader)\n    \n    scheduler.step(val_loss)\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        patience_counter = 0\n        best_state = model.state_dict().copy()\n    else:\n        patience_counter += 1\n    \n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch+1}: Train={train_loss:.6f}, Val={val_loss:.6f}\")\n    if patience_counter >= 15: break\n\nmodel.load_state_dict(best_state)\nprint(f\"\\n✓ Training time: {time.time() - start_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =====================================================================\n# COMPREHENSIVE EVALUATION WITH OVERFITTING DETECTION\n# =====================================================================\nprint(\"=\" * 70)\nprint(\"COMPREHENSIVE MODEL EVALUATION\")\nprint(\"=\" * 70)\n\nfrom scipy.stats import spearmanr\n\nmodel.eval()\nwith torch.no_grad():\n    y_pred_train = model(torch.FloatTensor(X_train_seq).to(DEVICE)).cpu().numpy()\n    y_pred_val = model(torch.FloatTensor(X_val_seq).to(DEVICE)).cpu().numpy()\n    y_pred_test = model(torch.FloatTensor(X_test_seq).to(DEVICE)).cpu().numpy()\n\ndef comprehensive_metrics(y_true, y_pred, set_name):\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse = np.sqrt(mse)\n    mae = np.mean(np.abs(y_true - y_pred))\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    r2 = 1 - (ss_res / (ss_tot + 1e-10))\n    ic, _ = spearmanr(y_true, y_pred)\n    direction_acc = np.mean(np.sign(y_true) == np.sign(y_pred))\n    strategy_returns = y_true * np.sign(y_pred)\n    sharpe = (np.mean(strategy_returns) / (np.std(strategy_returns) + 1e-10)) * np.sqrt(252*24*60)\n    downside = strategy_returns[strategy_returns < 0]\n    downside_std = np.std(downside) if len(downside) > 0 else 1e-10\n    sortino = (np.mean(strategy_returns) / (downside_std + 1e-10)) * np.sqrt(252*24*60)\n    cumulative = np.cumsum(strategy_returns)\n    running_max = np.maximum.accumulate(cumulative)\n    max_dd = np.min(cumulative - running_max)\n    profits = strategy_returns[strategy_returns > 0].sum()\n    losses = abs(strategy_returns[strategy_returns < 0].sum())\n    profit_factor = profits / (losses + 1e-10)\n    \n    print(f\"\\n{set_name} METRICS:\")\n    print(f\"  MSE: {mse:.8f} | RMSE: {rmse:.8f} | MAE: {mae:.8f}\")\n    print(f\"  R²: {r2:.6f} | IC: {ic:.6f} | Dir Acc: {direction_acc:.4%}\")\n    print(f\"  Sharpe: {sharpe:.4f} | Sortino: {sortino:.4f} | MaxDD: {max_dd:.6f}\")\n    \n    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2, 'ic': ic,\n            'direction_acc': direction_acc, 'sharpe': sharpe, 'sortino': sortino,\n            'max_dd': max_dd, 'profit_factor': profit_factor}\n\ntrain_metrics = comprehensive_metrics(y_train_seq, y_pred_train, \"TRAIN\")\nval_metrics = comprehensive_metrics(y_val_seq, y_pred_val, \"VALIDATION\")\ntest_metrics = comprehensive_metrics(y_test_seq, y_pred_test, \"TEST\")\n\n# OVERFITTING DETECTION\nprint(\"\\n\" + \"=\" * 70)\nprint(\"OVERFITTING ANALYSIS\")\nprint(\"=\" * 70)\n\ntrain_val_gap = train_metrics['sharpe'] - val_metrics['sharpe']\nval_test_gap = val_metrics['sharpe'] - test_metrics['sharpe']\ntrain_test_gap = train_metrics['sharpe'] - test_metrics['sharpe']\ndir_gap = train_metrics['direction_acc'] - test_metrics['direction_acc']\nr2_gap = train_metrics['r2'] - test_metrics['r2']\n\nprint(f\"\\nSharpe Gaps: Train-Val={train_val_gap:+.2f} | Val-Test={val_test_gap:+.2f} | Train-Test={train_test_gap:+.2f}\")\nprint(f\"Dir Acc Gap: {dir_gap:+.4%} | R² Gap: {r2_gap:+.6f}\")\n\noverfitting_score = sum([train_val_gap > 2, val_test_gap > 1, train_test_gap > 3, dir_gap > 0.05, r2_gap > 0.1])\nif overfitting_score == 0: print(\"\\n✓ NO OVERFITTING DETECTED\")\nelif overfitting_score <= 2: print(\"\\n⚠️ MILD OVERFITTING\")\nelse: print(\"\\n❌ SEVERE OVERFITTING\")\n\n# DATA LEAKAGE VALIDATION (FIXED: use <= for boundary condition)\nprint(\"\\n\" + \"=\" * 70)\nprint(\"DATA LEAKAGE VALIDATION\")\nprint(\"=\" * 70)\ntrain_max = train_df[\"open_time\"].max()\nval_min = val_df[\"open_time\"].min()\ntest_min = test_df[\"open_time\"].min()\n# Use <= because boundary rows at same timestamp belong to different symbols (no overlap)\ntemporal_ok = train_max <= val_min and val_min <= test_min\nprint(f\"Temporal Order: Train<=Val<=Test = {'✓ CORRECT' if temporal_ok else '❌ LEAKAGE'}\")\nprint(f\"Dir Acc {test_metrics['direction_acc']:.2%}: {'✓ Realistic' if test_metrics['direction_acc'] < 0.55 else '⚠️ High'}\")\nprint(f\"Sharpe {test_metrics['sharpe']:.2f}: {'✓ Realistic' if abs(test_metrics['sharpe']) < 3 else '⚠️ High'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export ONNX with opset 15 - save directly to trained/ directory\nonnx_path = TRAINED_DIR / \"cnn_model.onnx\"\ndummy = torch.randn(1, SEQUENCE_LENGTH, num_features).to(DEVICE)\ntorch.onnx.export(model, dummy, str(onnx_path), input_names=[\"input\"], output_names=[\"output\"],\n                  dynamic_axes={\"input\":{0:\"batch\"}, \"output\":{0:\"batch\"}}, opset_version=15)\n\nimport onnx\nonnx.checker.check_model(onnx.load(str(onnx_path)))\nprint(f\"✓ ONNX saved: {onnx_path}\")\n\nmetadata = {\n    \"model_type\": \"cnn\", \"sequence_length\": SEQUENCE_LENGTH, \"num_features\": num_features, \"onnx_opset\": 15,\n    \"train_metrics\": train_metrics, \"val_metrics\": val_metrics, \"test_metrics\": test_metrics,\n    \"overfitting_score\": int(overfitting_score),\n    \"temporal_ordering_valid\": bool(temporal_ok)\n}\nwith open(TRAINED_DIR / \"cnn_metadata.json\", \"w\") as f:\n    json.dump(metadata, f, indent=2, default=str)\ntorch.save(model.state_dict(), TRAINED_DIR / \"cnn_model.pt\")\nprint(\"\\n✓ CNN TRAINING COMPLETE!\")"
  }
 ]
}