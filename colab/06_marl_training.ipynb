{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# MARL Training Pipeline\n\n## ‚öôÔ∏è Runtime: L4 GPU (~18 CU)\n**Menu: Runtime ‚Üí Change runtime type ‚Üí L4 GPU**\n\n## Anti-Leakage Guarantees\n1. **Per-Symbol Temporal Split** - Each symbol split independently\n2. **RL Trained on Train Data ONLY** - No information from val/test\n3. **Proper Agent Isolation** - Agents communicate only through messages\n4. **Centralized Critic, Decentralized Actors** - CTDE paradigm\n\n## Output\n- `trained/marl_agent_0.onnx` to `trained/marl_agent_4.onnx` - 5 agent networks\n- `trained/marl_agent_0.pt` to `trained/marl_agent_4.pt`\n- `trained/marl_metadata.json`\n\n## Note\nTraining takes 5-7 hours due to 150 episodes x multi-agent coordination"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available(): print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch onnx onnxruntime-gpu pandas numpy scikit-learn scipy requests tqdm\n",
    "print(\"‚úì Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import RobustScaler\nfrom collections import deque\nfrom pathlib import Path\nimport json, time, random, warnings\nwarnings.filterwarnings('ignore')\n\nTRAINED_DIR = Path(\"trained\")\nTRAINED_DIR.mkdir(parents=True, exist_ok=True)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {DEVICE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import requests\nfrom datetime import datetime, timedelta\nfrom tqdm.notebook import tqdm\n\ndef fetch_klines_sync(symbol, days=90):\n    base_url = \"https://api.binance.com/api/v3/klines\"\n    end_time = datetime.utcnow()\n    start_time = end_time - timedelta(days=days)\n    all_data = []\n    current = start_time\n    while current < end_time:\n        params = {\"symbol\": symbol, \"interval\": \"1m\",\n                  \"startTime\": int(current.timestamp()*1000),\n                  \"endTime\": int(min(current+timedelta(days=1), end_time).timestamp()*1000), \"limit\": 1440}\n        try:\n            resp = requests.get(base_url, params=params, timeout=30)\n            data = resp.json()\n            if isinstance(data, list): all_data.extend(data)\n        except: pass\n        current += timedelta(days=1)\n        time.sleep(0.1)\n    if not all_data: return pd.DataFrame()\n    cols = [\"open_time\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"close_time\",\"quote_volume\",\"trades\",\"taker_buy_base\",\"taker_buy_quote\",\"ignore\"]\n    df = pd.DataFrame(all_data, columns=cols)\n    df[\"open_time\"] = pd.to_datetime(df[\"open_time\"], unit=\"ms\")\n    for c in [\"open\",\"high\",\"low\",\"close\",\"volume\",\"quote_volume\",\"taker_buy_base\",\"taker_buy_quote\"]: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    df[\"symbol\"] = symbol\n    return df.drop_duplicates(subset=[\"open_time\"]).sort_values(\"open_time\")\n\ndef calculate_comprehensive_features(df):\n    \"\"\"Calculate ~150 institutional-grade crypto features\"\"\"\n    df = df.copy()\n    ann_factor = np.sqrt(252 * 24 * 60)\n\n    # 1. RETURNS & PRICE ACTION\n    df[\"log_return\"] = np.log(df[\"close\"] / df[\"close\"].shift(1))\n    df[\"return_1\"] = df[\"close\"].pct_change(1)\n    for w in [5, 10, 20, 50, 100, 200]:\n        df[f\"return_{w}\"] = df[\"close\"].pct_change(w)\n    for w in [20, 50]:\n        vol = df[\"log_return\"].rolling(w).std()\n        df[f\"sharpe_{w}\"] = df[f\"return_{w}\"] / (vol * np.sqrt(w) + 1e-10)\n\n    # 2. VOLATILITY (multiple estimators)\n    for w in [5, 10, 20, 50, 100]:\n        df[f\"volatility_{w}\"] = df[\"log_return\"].rolling(w).std() * ann_factor\n    for w in [20, 50]:\n        log_hl = np.log(df[\"high\"] / df[\"low\"])\n        df[f\"parkinson_vol_{w}\"] = np.sqrt((1/(4*np.log(2))) * (log_hl**2).rolling(w).mean()) * ann_factor\n        log_co = np.log(df[\"close\"] / df[\"open\"])\n        gk = 0.5 * log_hl**2 - (2*np.log(2) - 1) * log_co**2\n        df[f\"gk_vol_{w}\"] = np.sqrt(gk.rolling(w).mean().abs()) * ann_factor\n    for w in [14, 20, 50]:\n        tr = pd.concat([df[\"high\"] - df[\"low\"], abs(df[\"high\"] - df[\"close\"].shift(1)), abs(df[\"low\"] - df[\"close\"].shift(1))], axis=1).max(axis=1)\n        df[f\"atr_{w}\"] = tr.rolling(w).mean()\n        df[f\"atr_pct_{w}\"] = df[f\"atr_{w}\"] / df[\"close\"] * 100\n    df[\"vol_regime\"] = df[\"volatility_20\"] / (df[\"volatility_100\"] + 1e-10)\n\n    # 3. VOLUME (CVD, VWAP, trades)\n    for w in [5, 10, 20, 50]:\n        df[f\"volume_ma_{w}\"] = df[\"volume\"].rolling(w).mean()\n    df[\"rvol_20\"] = df[\"volume\"] / (df[\"volume\"].rolling(20).mean() + 1e-10)\n    df[\"volume_zscore\"] = (df[\"volume\"] - df[\"volume\"].rolling(50).mean()) / (df[\"volume\"].rolling(50).std() + 1e-10)\n    typical_price = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\n    for w in [20, 50]:\n        cum_vol = df[\"volume\"].rolling(w).sum()\n        cum_tp_vol = (typical_price * df[\"volume\"]).rolling(w).sum()\n        df[f\"vwap_dist_{w}\"] = (df[\"close\"] - cum_tp_vol/(cum_vol+1e-10)) / (cum_tp_vol/(cum_vol+1e-10)+1e-10) * 100\n    volume_delta = df[\"taker_buy_base\"] - (df[\"volume\"] - df[\"taker_buy_base\"])\n    for w in [10, 20, 50]:\n        df[f\"cvd_{w}\"] = volume_delta.rolling(w).sum()\n        df[f\"cvd_norm_{w}\"] = df[f\"cvd_{w}\"] / (df[\"volume\"].rolling(w).sum() + 1e-10)\n    df[\"dollar_vol_ratio\"] = df[\"quote_volume\"] / (df[\"quote_volume\"].rolling(20).mean() + 1e-10)\n\n    # 4. MICROSTRUCTURE\n    df[\"spread_bps\"] = (df[\"high\"] - df[\"low\"]) / df[\"close\"] * 10000\n    df[\"ofi\"] = df[\"taker_buy_base\"] / (df[\"volume\"] + 1e-10)\n    for w in [10, 20, 50]:\n        df[f\"buy_pressure_{w}\"] = df[\"taker_buy_base\"].rolling(w).sum() / (df[\"volume\"].rolling(w).sum() + 1e-10)\n    df[\"amihud\"] = abs(df[\"return_1\"]) / (df[\"quote_volume\"] / 1e6 + 1e-10)\n\n    # 5. MOMENTUM (MACD, RSI, ADX, etc.)\n    for w in [5, 10, 20, 50, 100]:\n        df[f\"ma_dist_{w}\"] = (df[\"close\"] - df[\"close\"].rolling(w).mean()) / df[\"close\"].rolling(w).mean() * 100\n    ema12 = df[\"close\"].ewm(span=12, adjust=False).mean()\n    ema26 = df[\"close\"].ewm(span=26, adjust=False).mean()\n    df[\"macd\"] = ema12 - ema26\n    df[\"macd_signal\"] = df[\"macd\"].ewm(span=9, adjust=False).mean()\n    df[\"macd_hist\"] = df[\"macd\"] - df[\"macd_signal\"]\n    for w in [7, 14, 21]:\n        delta = df[\"close\"].diff()\n        gain = delta.where(delta > 0, 0).rolling(w).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(w).mean()\n        df[f\"rsi_{w}\"] = 100 - (100 / (1 + gain/(loss+1e-10)))\n        df[f\"rsi_{w}_norm\"] = (df[f\"rsi_{w}\"] - 50) / 50\n    rsi14 = df[\"rsi_14\"]\n    rsi_min, rsi_max = rsi14.rolling(14).min(), rsi14.rolling(14).max()\n    df[\"stoch_rsi\"] = (rsi14 - rsi_min) / (rsi_max - rsi_min + 1e-10)\n    for w in [14, 21]:\n        highest, lowest = df[\"high\"].rolling(w).max(), df[\"low\"].rolling(w).min()\n        df[f\"williams_r_{w}\"] = -100 * (highest - df[\"close\"]) / (highest - lowest + 1e-10)\n    for w in [14, 20]:\n        plus_dm = df[\"high\"].diff().where(lambda x: x > 0, 0)\n        minus_dm = (-df[\"low\"].diff()).where(lambda x: x > 0, 0)\n        tr = pd.concat([df[\"high\"]-df[\"low\"], abs(df[\"high\"]-df[\"close\"].shift(1)), abs(df[\"low\"]-df[\"close\"].shift(1))], axis=1).max(axis=1)\n        atr = tr.rolling(w).mean()\n        plus_di = 100 * (plus_dm.rolling(w).mean() / (atr + 1e-10))\n        minus_di = 100 * (minus_dm.rolling(w).mean() / (atr + 1e-10))\n        df[f\"adx_{w}\"] = (100 * abs(plus_di - minus_di) / (plus_di + minus_di + 1e-10)).rolling(w).mean()\n    tp = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\n    df[\"cci_20\"] = (tp - tp.rolling(20).mean()) / (0.015 * tp.rolling(20).std() + 1e-10)\n\n    # 6. MEAN REVERSION (Bollinger, z-scores)\n    for w in [20, 50]:\n        ma, std = df[\"close\"].rolling(w).mean(), df[\"close\"].rolling(w).std()\n        df[f\"bb_width_{w}\"] = (4 * std) / ma * 100\n        df[f\"bb_position_{w}\"] = (df[\"close\"] - (ma - 2*std)) / (4*std + 1e-10)\n        df[f\"price_zscore_{w}\"] = (df[\"close\"] - ma) / (std + 1e-10)\n\n    # 7. TIME FEATURES\n    hour = df[\"open_time\"].dt.hour\n    dow = df[\"open_time\"].dt.dayofweek\n    df[\"hour_sin\"] = np.sin(2 * np.pi * hour / 24)\n    df[\"hour_cos\"] = np.cos(2 * np.pi * hour / 24)\n    df[\"dow_sin\"] = np.sin(2 * np.pi * dow / 7)\n    df[\"dow_cos\"] = np.cos(2 * np.pi * dow / 7)\n    df[\"is_asia\"] = ((hour >= 0) & (hour < 8)).astype(int)\n    df[\"is_europe\"] = ((hour >= 7) & (hour < 16)).astype(int)\n    df[\"is_us\"] = ((hour >= 13) & (hour < 22)).astype(int)\n    df[\"is_weekend\"] = (dow >= 5).astype(int)\n\n    # 8. STATISTICAL\n    for w in [20, 50]:\n        df[f\"skewness_{w}\"] = df[\"log_return\"].rolling(w).skew()\n        df[f\"kurtosis_{w}\"] = df[\"log_return\"].rolling(w).kurt()\n\n    # 9. PRICE PATTERNS\n    for w in [20, 50, 100]:\n        highest, lowest = df[\"high\"].rolling(w).max(), df[\"low\"].rolling(w).min()\n        df[f\"dist_from_high_{w}\"] = (df[\"close\"] - highest) / highest * 100\n        df[f\"dist_from_low_{w}\"] = (df[\"close\"] - lowest) / lowest * 100\n        df[f\"range_position_{w}\"] = (df[\"close\"] - lowest) / (highest - lowest + 1e-10)\n\n    return df\n\ndef get_feature_columns(df):\n    exclude = [\"open_time\",\"close_time\",\"symbol\",\"ignore\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"quote_volume\",\"trades\",\"taker_buy_base\",\"taker_buy_quote\",\"hour\",\"day_of_week\"]\n    return [c for c in df.columns if c not in exclude and not c.startswith(\"target_\")]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SYMBOLS = [\"BTCUSDT\", \"ETHUSDT\", \"BNBUSDT\", \"SOLUSDT\"]\nprint(\"Collecting data...\")\nall_data = []\nfor sym in tqdm(SYMBOLS):\n    df = fetch_klines_sync(sym, days=90)\n    if len(df) > 0:\n        all_data.append(df)\n        print(f\"  ‚úì {sym}: {len(df):,} rows\")\n\nif not all_data: raise ValueError(\"No data collected!\")\nraw_data = pd.concat(all_data, ignore_index=True)\nprint(f\"\\n‚úì Total: {len(raw_data):,} rows\")\n\n# Per-symbol split - MARL uses ONLY training data with comprehensive features\ntrain_dfs = []\nfor sym in raw_data[\"symbol\"].unique():\n    sdf = raw_data[raw_data[\"symbol\"]==sym].copy().sort_values(\"open_time\").reset_index(drop=True)\n    sdf = calculate_comprehensive_features(sdf)  # ~150 features\n    sdf = sdf.replace([np.inf,-np.inf], np.nan).iloc[200:].dropna()  # Extended warmup\n    n = len(sdf)\n    train_end = int(n * 0.70)\n    train_dfs.append(sdf.iloc[:train_end])\n    print(f\"{sym}: {train_end:,} train rows\")\n\ntrain_df = pd.concat(train_dfs).sort_values(\"open_time\").reset_index(drop=True)\nprint(f\"\\n‚úì Total train: {len(train_df):,}\")\nprint(f\"‚úì Features: {len(get_feature_columns(train_df))}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare RL data\n",
    "feature_cols = get_feature_columns(train_df)\n",
    "ohlcv_cols = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "\n",
    "scaler = RobustScaler()\n",
    "rl_features = scaler.fit_transform(train_df[feature_cols].values)\n",
    "rl_ohlcv = train_df[ohlcv_cols].values\n",
    "\n",
    "print(f\"RL Features: {rl_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARL Agent Network\n",
    "class MARLAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim=1, hidden_dim=128, message_dim=32, n_agents=5):\n",
    "        super().__init__()\n",
    "        self.state_enc = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU()\n",
    "        )\n",
    "        self.msg_enc = nn.Sequential(\n",
    "            nn.Linear(message_dim * (n_agents - 1), hidden_dim // 2), nn.ReLU()\n",
    "        )\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim), nn.Tanh()\n",
    "        )\n",
    "        self.msg_gen = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, message_dim), nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, state, messages):\n",
    "        state_emb = self.state_enc(state)\n",
    "        msg_emb = self.msg_enc(messages.flatten(start_dim=-2))\n",
    "        combined = torch.cat([state_emb, msg_emb], dim=-1)\n",
    "        action = self.policy(combined)\n",
    "        msg_out = self.msg_gen(state_emb)\n",
    "        return action, msg_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trading Environment for MARL\n",
    "class TradingEnvMARL:\n",
    "    def __init__(self, data, features, initial_balance=100000, transaction_cost=0.0005):\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0.0\n",
    "        self.step_idx = 0\n",
    "        self.returns = []\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        market = self.features[self.step_idx]\n",
    "        portfolio = np.array([\n",
    "            self.position,\n",
    "            self.balance / self.initial_balance - 1,\n",
    "            np.mean(self.returns[-20:]) if self.returns else 0,\n",
    "            np.std(self.returns[-20:]) if len(self.returns) > 1 else 0\n",
    "        ])\n",
    "        return np.concatenate([market, portfolio])\n",
    "\n",
    "    def step(self, action):\n",
    "        target_pos = float(np.clip(action[0], -1, 1))\n",
    "        pos_change = target_pos - self.position\n",
    "        current_price = self.data[self.step_idx, 3]\n",
    "        cost = abs(pos_change) * current_price * self.transaction_cost\n",
    "\n",
    "        self.step_idx += 1\n",
    "        done = self.step_idx >= len(self.data) - 1\n",
    "\n",
    "        if not done:\n",
    "            next_price = self.data[self.step_idx, 3]\n",
    "            ret = (next_price - current_price) / current_price\n",
    "            pnl = self.position * ret * self.balance - cost\n",
    "            self.balance += pnl\n",
    "            step_ret = pnl / self.initial_balance\n",
    "            self.returns.append(step_ret)\n",
    "            self.position = target_pos\n",
    "\n",
    "            if len(self.returns) > 1:\n",
    "                reward = np.mean(self.returns[-20:]) / (np.std(self.returns[-20:]) + 1e-8)\n",
    "            else:\n",
    "                reward = step_ret * 100\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        return self._get_state() if not done else np.zeros_like(self._get_state()), reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training MARL\nprint(\"=\"*60)\nprint(\"TRAINING MARL (CUDA)\")\nprint(\"=\"*60)\nprint(\"WARNING: This will take 5-7 hours\")\nprint(\"=\"*60)\n\nn_agents = 5\nmessage_dim = 32\nstate_dim = rl_features.shape[1] + 4\n\nagents = [MARLAgent(state_dim, message_dim=message_dim, n_agents=n_agents).to(DEVICE) for _ in range(n_agents)]\noptimizers = [torch.optim.Adam(a.parameters(), lr=3e-4) for a in agents]\n\n# Centralized critic\ncritic = nn.Sequential(\n    nn.Linear(n_agents * state_dim + n_agents, 256), nn.ReLU(),\n    nn.Linear(256, 256), nn.ReLU(),\n    nn.Linear(256, n_agents)\n).to(DEVICE)\ncritic_opt = torch.optim.Adam(critic.parameters(), lr=3e-4)\n\nbuffer = deque(maxlen=50000)\nbatch_size = 256\nepisodes = 150\n\n# Track episode metrics for evaluation\nepisode_returns = []\nepisode_sharpes = []\n\nstart_time = time.time()\nfor ep in range(episodes):\n    env = TradingEnvMARL(rl_ohlcv, rl_features)\n    base_state = env.reset()\n\n    # Each agent gets slightly different observation (noise)\n    states = np.array([base_state + np.random.normal(0, 0.01, size=base_state.shape) for _ in range(n_agents)])\n    messages = np.zeros((n_agents, message_dim))\n\n    ep_reward = 0\n\n    while True:\n        actions = []\n        new_messages = []\n\n        for i, agent in enumerate(agents):\n            other_msgs = np.stack([messages[j] for j in range(n_agents) if j != i])\n            state_t = torch.FloatTensor(states[i]).unsqueeze(0).to(DEVICE)\n            msgs_t = torch.FloatTensor(other_msgs).unsqueeze(0).to(DEVICE)\n\n            with torch.no_grad():\n                action, msg = agent(state_t, msgs_t)\n\n            action = action.cpu().numpy()[0] + np.random.normal(0, 0.1, size=(1,))\n            action = np.clip(action, -1, 1)\n            actions.append(action)\n            new_messages.append(msg.cpu().numpy()[0])\n\n        messages = np.array(new_messages)\n\n        # Aggregate actions (mean)\n        agg_action = np.mean(actions, axis=0)\n        next_base_state, reward, done = env.step(agg_action)\n\n        next_states = np.array([next_base_state + np.random.normal(0, 0.01, size=next_base_state.shape) for _ in range(n_agents)])\n        rewards = np.full(n_agents, reward)\n\n        buffer.append((states.copy(), np.array(actions), rewards, next_states.copy(), done))\n\n        states = next_states\n        ep_reward += reward\n\n        # Training\n        if len(buffer) >= batch_size:\n            batch = random.sample(buffer, batch_size)\n            b_states, b_actions, b_rewards, _, _ = zip(*batch)\n\n            b_states = torch.FloatTensor(np.array(b_states)).to(DEVICE)\n            b_actions = torch.FloatTensor(np.array(b_actions)).to(DEVICE)\n            b_rewards = torch.FloatTensor(np.array(b_rewards)).to(DEVICE)\n\n            # Critic update\n            critic_input = torch.cat([b_states.view(batch_size, -1), b_actions.view(batch_size, -1)], dim=-1)\n            q_values = critic(critic_input)\n            critic_loss = nn.MSELoss()(q_values, b_rewards)\n\n            critic_opt.zero_grad()\n            critic_loss.backward()\n            critic_opt.step()\n\n        if done:\n            break\n\n    # Track metrics\n    total_ret = (env.balance - env.initial_balance) / env.initial_balance\n    episode_returns.append(total_ret)\n    if len(env.returns) > 1:\n        ep_sharpe = np.mean(env.returns) / (np.std(env.returns) + 1e-10) * np.sqrt(252*24*60)\n        episode_sharpes.append(ep_sharpe)\n\n    if (ep + 1) % 20 == 0:\n        print(f\"Episode {ep+1}/{episodes} - Return: {total_ret:.2%}\")\n\ntrain_time = time.time() - start_time\nprint(f\"\\n‚úì Training time: {train_time/3600:.2f} hours\")"
  },
  {
   "cell_type": "code",
   "source": "# Comprehensive MARL Evaluation with Overfitting Detection\nfrom scipy.stats import spearmanr\n\ndef comprehensive_marl_metrics(env, episode_returns, episode_sharpes, model_name=\"MARL\"):\n    \"\"\"Calculate comprehensive metrics for MARL model evaluation\"\"\"\n    returns = np.array(env.returns)\n    \n    if len(returns) == 0:\n        print(f\"‚ö†Ô∏è No returns to evaluate for {model_name}\")\n        return {}\n    \n    ann_factor = np.sqrt(252 * 24 * 60)\n    \n    # Basic metrics\n    total_return = (env.balance - env.initial_balance) / env.initial_balance\n    mean_return = np.mean(returns)\n    std_return = np.std(returns)\n    \n    # Risk-adjusted metrics\n    sharpe = (mean_return / (std_return + 1e-10)) * ann_factor\n    \n    # Sortino ratio\n    downside_returns = returns[returns < 0]\n    downside_std = np.std(downside_returns) if len(downside_returns) > 0 else 1e-10\n    sortino = (mean_return / (downside_std + 1e-10)) * ann_factor\n    \n    # Maximum Drawdown\n    cumulative = np.cumsum(returns)\n    running_max = np.maximum.accumulate(cumulative)\n    drawdowns = running_max - cumulative\n    max_dd = np.max(drawdowns) if len(drawdowns) > 0 else 0\n    \n    # Calmar ratio\n    calmar = total_return / (max_dd + 1e-10) if max_dd > 0 else 0\n    \n    # Win rate & profit factor\n    wins = returns[returns > 0]\n    losses = returns[returns < 0]\n    win_rate = len(wins) / len(returns) if len(returns) > 0 else 0\n    profit_factor = abs(np.sum(wins) / (np.sum(losses) + 1e-10)) if len(losses) > 0 else 0\n    \n    # Tail risk metrics\n    var_95 = np.percentile(returns, 5)\n    var_99 = np.percentile(returns, 1)\n    cvar_95 = np.mean(returns[returns <= var_95]) if len(returns[returns <= var_95]) > 0 else 0\n    \n    # Training stability metrics\n    if len(episode_returns) > 10:\n        first_third = episode_returns[:len(episode_returns)//3]\n        last_third = episode_returns[-len(episode_returns)//3:]\n        learning_improvement = np.mean(last_third) - np.mean(first_third)\n        return_volatility = np.std(episode_returns)\n    else:\n        learning_improvement = 0\n        return_volatility = 0\n    \n    # Sharpe stability across episodes\n    if len(episode_sharpes) > 10:\n        sharpe_std = np.std(episode_sharpes)\n        sharpe_mean = np.mean(episode_sharpes)\n        sharpe_stability = 1 - (sharpe_std / (abs(sharpe_mean) + 1e-10))\n    else:\n        sharpe_stability = 0\n    \n    metrics = {\n        \"total_return\": total_return,\n        \"mean_return\": mean_return,\n        \"std_return\": std_return,\n        \"sharpe\": sharpe,\n        \"sortino\": sortino,\n        \"max_drawdown\": max_dd,\n        \"calmar\": calmar,\n        \"win_rate\": win_rate,\n        \"profit_factor\": profit_factor,\n        \"var_95\": var_95,\n        \"var_99\": var_99,\n        \"cvar_95\": cvar_95,\n        \"learning_improvement\": learning_improvement,\n        \"return_volatility_across_episodes\": return_volatility,\n        \"sharpe_stability\": sharpe_stability,\n        \"n_trades\": len(returns),\n        \"n_episodes\": len(episode_returns)\n    }\n    \n    return metrics\n\ndef print_marl_metrics(metrics, model_name=\"MARL\"):\n    \"\"\"Print formatted metrics\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"{model_name} COMPREHENSIVE EVALUATION\")\n    print(f\"{'='*60}\")\n    \n    print(f\"\\nüìä RETURN METRICS:\")\n    print(f\"  Total Return:     {metrics['total_return']:.4%}\")\n    print(f\"  Mean Return:      {metrics['mean_return']:.6f}\")\n    print(f\"  Std Return:       {metrics['std_return']:.6f}\")\n    \n    print(f\"\\nüìà RISK-ADJUSTED METRICS:\")\n    print(f\"  Sharpe Ratio:     {metrics['sharpe']:.4f}\")\n    print(f\"  Sortino Ratio:    {metrics['sortino']:.4f}\")\n    print(f\"  Calmar Ratio:     {metrics['calmar']:.4f}\")\n    \n    print(f\"\\nüìâ RISK METRICS:\")\n    print(f\"  Max Drawdown:     {metrics['max_drawdown']:.4%}\")\n    print(f\"  VaR 95%:          {metrics['var_95']:.6f}\")\n    print(f\"  VaR 99%:          {metrics['var_99']:.6f}\")\n    print(f\"  CVaR 95%:         {metrics['cvar_95']:.6f}\")\n    \n    print(f\"\\nüéØ TRADING METRICS:\")\n    print(f\"  Win Rate:         {metrics['win_rate']:.2%}\")\n    print(f\"  Profit Factor:    {metrics['profit_factor']:.4f}\")\n    print(f\"  Total Trades:     {metrics['n_trades']:,}\")\n    \n    print(f\"\\nüî¨ TRAINING STABILITY:\")\n    print(f\"  Learning Improvement: {metrics['learning_improvement']:.4%}\")\n    print(f\"  Return Volatility:    {metrics['return_volatility_across_episodes']:.4f}\")\n    print(f\"  Sharpe Stability:     {metrics['sharpe_stability']:.4f}\")\n    print(f\"  Episodes Trained:     {metrics['n_episodes']}\")\n\ndef overfitting_analysis_marl(metrics, episode_returns, episode_sharpes):\n    \"\"\"Analyze potential overfitting in MARL model\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(\"MARL OVERFITTING ANALYSIS\")\n    print(f\"{'='*60}\")\n    \n    warnings = []\n    \n    # Check for unrealistic metrics\n    if metrics['sharpe'] > 3.0:\n        warnings.append(f\"‚ö†Ô∏è HIGH SHARPE ({metrics['sharpe']:.2f}) - Possible overfitting\")\n    \n    if metrics['win_rate'] > 0.60:\n        warnings.append(f\"‚ö†Ô∏è HIGH WIN RATE ({metrics['win_rate']:.2%}) - Check for data leakage\")\n    \n    if metrics['sharpe_stability'] < 0.3:\n        warnings.append(f\"‚ö†Ô∏è LOW SHARPE STABILITY ({metrics['sharpe_stability']:.2f}) - Inconsistent across episodes\")\n    \n    if metrics['profit_factor'] > 3.0:\n        warnings.append(f\"‚ö†Ô∏è HIGH PROFIT FACTOR ({metrics['profit_factor']:.2f}) - Suspiciously good\")\n    \n    if metrics['max_drawdown'] < 0.01:\n        warnings.append(f\"‚ö†Ô∏è VERY LOW DRAWDOWN ({metrics['max_drawdown']:.4%}) - Unrealistic\")\n    \n    # Learning curve analysis\n    if len(episode_returns) > 20:\n        # Check for learning plateau or decline\n        last_20 = episode_returns[-20:]\n        if np.mean(last_20) < np.mean(episode_returns[:-20]):\n            warnings.append(\"‚ö†Ô∏è PERFORMANCE DECLINE - Model may be overfitting in later episodes\")\n        \n        # Check for unstable learning\n        if np.std(last_20) > np.mean(np.abs(last_20)):\n            warnings.append(\"‚ö†Ô∏è UNSTABLE LEARNING - High variance in recent episodes\")\n    \n    # Multi-agent coordination check\n    if metrics['learning_improvement'] < 0:\n        warnings.append(\"‚ö†Ô∏è NEGATIVE LEARNING - Model got worse during training\")\n    \n    if len(warnings) == 0:\n        print(\"‚úÖ No obvious overfitting signals detected\")\n        print(\"   - Sharpe ratio in realistic range\")\n        print(\"   - Win rate not suspiciously high\")\n        print(\"   - Learning curve shows improvement\")\n        print(\"   - Multi-agent coordination appears stable\")\n    else:\n        for w in warnings:\n            print(w)\n    \n    # Final verdict\n    print(f\"\\nüìã VERDICT:\")\n    if len(warnings) <= 1:\n        print(\"‚úÖ MARL model appears well-calibrated for production\")\n    elif len(warnings) <= 3:\n        print(\"‚ö†Ô∏è Some concerns - recommend additional validation\")\n    else:\n        print(\"‚ùå Multiple overfitting signals - DO NOT deploy without investigation\")\n    \n    return warnings\n\n# Run MARL evaluation\nmarl_metrics = comprehensive_marl_metrics(env, episode_returns, episode_sharpes, \"MARL\")\nprint_marl_metrics(marl_metrics, \"MARL\")\noverfitting_warnings = overfitting_analysis_marl(marl_metrics, episode_returns, episode_sharpes)\n\n# Save metrics\nimport json\neval_metrics = {\n    **marl_metrics,\n    \"episode_returns_mean\": float(np.mean(episode_returns)),\n    \"episode_returns_std\": float(np.std(episode_returns)),\n    \"overfitting_warnings\": len(overfitting_warnings)\n}\nwith open(TRAINED_DIR / \"marl_evaluation.json\", \"w\") as f:\n    json.dump(eval_metrics, f, indent=2)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export MARL Agents to ONNX (FIXED: opset_version=15, save to trained/)\nimport onnx\n\nclass AgentActionExtractor(nn.Module):\n    def __init__(self, agent):\n        super().__init__()\n        self.agent = agent\n\n    def forward(self, state, messages):\n        action, _ = self.agent(state, messages)\n        return action\n\nfor i, agent in enumerate(agents):\n    extractor = AgentActionExtractor(agent)\n    extractor.eval()\n\n    dummy_state = torch.randn(1, state_dim).to(DEVICE)\n    dummy_msgs = torch.randn(1, n_agents - 1, message_dim).to(DEVICE)\n\n    # Save directly to trained/ directory\n    onnx_path = TRAINED_DIR / f\"marl_agent_{i}.onnx\"\n    torch.onnx.export(\n        extractor, (dummy_state, dummy_msgs), str(onnx_path),\n        input_names=[\"state\", \"messages\"], output_names=[\"action\"],\n        dynamic_axes={\"state\": {0: \"batch\"}, \"messages\": {0: \"batch\"}, \"action\": {0: \"batch\"}},\n        opset_version=15  # FIXED: Changed from 17 to 15 for Colab ONNX compatibility\n    )\n\n    onnx.checker.check_model(onnx.load(str(onnx_path)))\n    print(f\"‚úì Agent {i} saved: {onnx_path}\")\n\n    # Save PyTorch\n    torch.save(agent.state_dict(), TRAINED_DIR / f\"marl_agent_{i}.pt\")\n\n# Metadata with evaluation metrics\nmetadata = {\n    \"model_type\": \"marl\",\n    \"n_agents\": n_agents,\n    \"state_dim\": state_dim,\n    \"message_dim\": message_dim,\n    \"train_time_hours\": train_time / 3600,\n    \"evaluation\": eval_metrics\n}\nwith open(TRAINED_DIR / \"marl_metadata.json\", \"w\") as f:\n    json.dump(metadata, f, indent=2)\n\nprint(\"\\n‚úì MARL TRAINING COMPLETE!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}