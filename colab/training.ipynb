{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORPFlow - Training Pipeline (GPU A100)\n",
    "\n",
    "**IMPORTANTE: Este notebook implementa um pipeline LIVRE de Data Leakage**\n",
    "\n",
    "## Garantias Anti-Leakage:\n",
    "1. Split temporal POR SÍMBOLO (elimina cross-symbol contamination)\n",
    "2. Scaler fit APENAS nos dados de treino\n",
    "3. Features calculadas usando apenas dados passados (rolling windows backward-looking)\n",
    "4. Cada modelo treinado de forma independente\n",
    "5. Sem vazamento de informação entre train/val/test\n",
    "6. Sem bfill() que causaria leakage\n",
    "7. RL treinado APENAS com dados de treino\n",
    "\n",
    "## Correção v2.0:\n",
    "- **ANTES**: Global sort + index split → cross-symbol contamination nas bordas\n",
    "- **AGORA**: Split por símbolo INDEPENDENTE → cada símbolo tem seu próprio split temporal\n",
    "\n",
    "## Modelos:\n",
    "- ML: LightGBM, XGBoost\n",
    "- DL: LSTM, CNN  \n",
    "- RL: D4PG+EVT, MARL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 21 02:49:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   35C    P0             61W /  400W |   25621MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "PyTorch: 2.9.1+cu128\n",
      "CUDA disponível: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "Memória GPU: 85.2 GB\n"
     ]
    }
   ],
   "source": [
    "# Verificar GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA disponível: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memória GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependências instaladas!\n"
     ]
    }
   ],
   "source": [
    "# Instalar dependências\n",
    "!pip install -q lightgbm xgboost torch torchvision --upgrade\n",
    "!pip install -q onnx onnxruntime-gpu onnxmltools skl2onnx\n",
    "!pip install -q pandas numpy scikit-learn scipy\n",
    "!pip install -q aiohttp tqdm pyyaml matplotlib seaborn\n",
    "!pip install -q python-binance\n",
    "\n",
    "print(\"Dependências instaladas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Diretórios\n",
    "DATA_DIR = Path(\"data\")\n",
    "TRAINED_DIR = Path(\"trained\")\n",
    "ONNX_DIR = TRAINED_DIR / \"onnx\"\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TRAINED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ONNX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Coleta de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "async def fetch_klines(symbol: str, interval: str = \"1m\", days: int = 90) -> pd.DataFrame:\n",
    "    \"\"\"Coleta dados históricos da Binance\"\"\"\n",
    "    base_url = \"https://api.binance.com/api/v3/klines\"\n",
    "    end_time = datetime.utcnow()\n",
    "    start_time = end_time - timedelta(days=days)\n",
    "\n",
    "    all_data = []\n",
    "    current = start_time\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        while current < end_time:\n",
    "            params = {\n",
    "                \"symbol\": symbol,\n",
    "                \"interval\": interval,\n",
    "                \"startTime\": int(current.timestamp() * 1000),\n",
    "                \"endTime\": int(min(current + timedelta(days=1), end_time).timestamp() * 1000),\n",
    "                \"limit\": 1440\n",
    "            }\n",
    "\n",
    "            async with session.get(base_url, params=params) as resp:\n",
    "                data = await resp.json()\n",
    "                all_data.extend(data)\n",
    "\n",
    "            current += timedelta(days=1)\n",
    "            await asyncio.sleep(0.1)  # Rate limit\n",
    "\n",
    "    columns = [\"open_time\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "               \"close_time\", \"quote_volume\", \"trades\", \"taker_buy_base\",\n",
    "               \"taker_buy_quote\", \"ignore\"]\n",
    "\n",
    "    df = pd.DataFrame(all_data, columns=columns)\n",
    "    df[\"open_time\"] = pd.to_datetime(df[\"open_time\"], unit=\"ms\")\n",
    "    df[\"close_time\"] = pd.to_datetime(df[\"close_time\"], unit=\"ms\")\n",
    "\n",
    "    for col in [\"open\", \"high\", \"low\", \"close\", \"volume\", \"quote_volume\",\n",
    "                \"taker_buy_base\", \"taker_buy_quote\"]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    df[\"symbol\"] = symbol\n",
    "    return df.drop_duplicates(subset=[\"open_time\"]).sort_values(\"open_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3883ae2e574a4880c8f276f269c687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Coletando dados:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTCUSDT: 90000 rows\n",
      "ETHUSDT: 90000 rows\n",
      "BNBUSDT: 90000 rows\n",
      "SOLUSDT: 90000 rows\n",
      "\n",
      "Total: 360000 rows\n"
     ]
    }
   ],
   "source": [
    "# Coletar dados\n",
    "SYMBOLS = [\"BTCUSDT\", \"ETHUSDT\", \"BNBUSDT\", \"SOLUSDT\"]\n",
    "DAYS = 90\n",
    "\n",
    "all_data = []\n",
    "for symbol in tqdm(SYMBOLS, desc=\"Coletando dados\"):\n",
    "    df = await fetch_klines(symbol, days=DAYS)\n",
    "    all_data.append(df)\n",
    "    print(f\"{symbol}: {len(df)} rows\")\n",
    "\n",
    "raw_data = pd.concat(all_data, ignore_index=True)\n",
    "print(f\"\\nTotal: {len(raw_data)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering (Sem Leakage)\n",
    "\n",
    "**IMPORTANTE:** Todas as features usam APENAS dados passados (rolling windows olham para trás)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(df: pd.DataFrame, windows: list = [5, 10, 20, 50, 100]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcula features usando APENAS dados passados.\n",
    "    Todas as rolling windows olham para trás, sem leakage.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Returns (olham para o passado)\n",
    "    df[\"return_1\"] = df[\"close\"].pct_change()\n",
    "    df[\"log_return\"] = np.log(df[\"close\"] / df[\"close\"].shift(1))\n",
    "\n",
    "    for w in windows:\n",
    "        df[f\"return_{w}\"] = df[\"close\"].pct_change(w)\n",
    "        df[f\"log_return_{w}\"] = np.log(df[\"close\"] / df[\"close\"].shift(w))\n",
    "\n",
    "    # Volatility (rolling olha para trás)\n",
    "    for w in windows:\n",
    "        df[f\"volatility_{w}\"] = df[\"log_return\"].rolling(window=w).std() * np.sqrt(252 * 24 * 60)\n",
    "\n",
    "        # Parkinson volatility\n",
    "        df[f\"parkinson_vol_{w}\"] = np.sqrt(\n",
    "            (1 / (4 * np.log(2))) *\n",
    "            ((np.log(df[\"high\"] / df[\"low\"]) ** 2).rolling(window=w).mean())\n",
    "        ) * np.sqrt(252 * 24 * 60)\n",
    "\n",
    "        # Garman-Klass volatility\n",
    "        log_hl = np.log(df[\"high\"] / df[\"low\"]) ** 2\n",
    "        log_co = np.log(df[\"close\"] / df[\"open\"]) ** 2\n",
    "        df[f\"gk_vol_{w}\"] = np.sqrt(\n",
    "            (0.5 * log_hl - (2 * np.log(2) - 1) * log_co).rolling(window=w).mean()\n",
    "        ) * np.sqrt(252 * 24 * 60)\n",
    "\n",
    "    # Momentum (olham para o passado)\n",
    "    for w in windows:\n",
    "        df[f\"momentum_{w}\"] = df[\"close\"] / df[\"close\"].shift(w) - 1\n",
    "        df[f\"roc_{w}\"] = (df[\"close\"] - df[\"close\"].shift(w)) / df[\"close\"].shift(w) * 100\n",
    "        df[f\"ma_{w}\"] = df[\"close\"].rolling(window=w).mean()\n",
    "        df[f\"ma_cross_{w}\"] = (df[\"close\"] - df[f\"ma_{w}\"]) / df[f\"ma_{w}\"]\n",
    "\n",
    "    # RSI (olha para o passado)\n",
    "    for w in [14, 21]:\n",
    "        delta = df[\"close\"].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=w).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=w).mean()\n",
    "        rs = gain / loss\n",
    "        df[f\"rsi_{w}\"] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # Order flow (olham para o passado)\n",
    "    df[\"spread_proxy\"] = (df[\"high\"] - df[\"low\"]) / df[\"close\"] * 10000\n",
    "    df[\"volume_imbalance\"] = (df[\"taker_buy_base\"] - (df[\"volume\"] - df[\"taker_buy_base\"])) / df[\"volume\"]\n",
    "    df[\"ofi\"] = df[\"taker_buy_base\"] / df[\"volume\"]\n",
    "\n",
    "    for w in windows:\n",
    "        df[f\"ofi_ma_{w}\"] = df[\"ofi\"].rolling(window=w).mean()\n",
    "        df[f\"ofi_std_{w}\"] = df[\"ofi\"].rolling(window=w).std()\n",
    "        df[f\"volume_ma_{w}\"] = df[\"volume\"].rolling(window=w).mean()\n",
    "        df[f\"volume_std_{w}\"] = df[\"volume\"].rolling(window=w).std()\n",
    "        df[f\"trades_ma_{w}\"] = df[\"trades\"].rolling(window=w).mean()\n",
    "\n",
    "    # Microstructure (olham para o passado)\n",
    "    df[\"amihud\"] = np.abs(df[\"log_return\"]) / df[\"quote_volume\"]\n",
    "    for w in windows:\n",
    "        df[f\"amihud_ma_{w}\"] = df[\"amihud\"].rolling(window=w).mean()\n",
    "        df[f\"kyle_lambda_{w}\"] = df[\"log_return\"].rolling(window=w).std() / df[\"volume\"].rolling(window=w).mean()\n",
    "\n",
    "    # Time features (sem leakage - são características do momento)\n",
    "    df[\"hour\"] = df[\"open_time\"].dt.hour\n",
    "    df[\"day_of_week\"] = df[\"open_time\"].dt.dayofweek\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
    "    df[\"dow_sin\"] = np.sin(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "    df[\"dow_cos\"] = np.cos(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_targets(df: pd.DataFrame, horizons: list = [1, 5, 15, 30]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcula targets (valores FUTUROS que queremos prever).\n",
    "    Usamos shift(-h) para pegar retorno futuro.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    for h in horizons:\n",
    "        # Retorno futuro (o que queremos prever)\n",
    "        df[f\"target_return_{h}\"] = df[\"close\"].shift(-h) / df[\"close\"] - 1\n",
    "        # Direção futura\n",
    "        df[f\"target_direction_{h}\"] = (df[f\"target_return_{h}\"] > 0).astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Retorna apenas colunas de features (exclui targets e metadata)\"\"\"\n",
    "    exclude_prefixes = [\"target_\", \"open_time\", \"close_time\", \"symbol\", \"ignore\"]\n",
    "    exclude_cols = [\"open\", \"high\", \"low\", \"close\", \"volume\", \"quote_volume\",\n",
    "                    \"trades\", \"taker_buy_base\", \"taker_buy_quote\", \"hour\", \"day_of_week\"]\n",
    "\n",
    "    feature_cols = []\n",
    "    for col in df.columns:\n",
    "        if any(col.startswith(p) for p in exclude_prefixes):\n",
    "            continue\n",
    "        if col in exclude_cols:\n",
    "            continue\n",
    "        feature_cols.append(col)\n",
    "\n",
    "    return feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando BTCUSDT: 90000 rows\n",
      "  -> 89870 rows após processamento\n",
      "Processando ETHUSDT: 90000 rows\n",
      "  -> 89870 rows após processamento\n",
      "Processando BNBUSDT: 90000 rows\n",
      "  -> 89870 rows após processamento\n",
      "Processando SOLUSDT: 90000 rows\n",
      "  -> 89870 rows após processamento\n",
      "\n",
      "Total processado: 359480 rows\n",
      "Símbolos: ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT']\n"
     ]
    }
   ],
   "source": [
    "# Processar cada símbolo e ARMAZENAR SEPARADAMENTE\n",
    "# NÃO concatenar aqui - faremos split por símbolo!\n",
    "processed_by_symbol = {}\n",
    "\n",
    "for symbol in raw_data[\"symbol\"].unique():\n",
    "    symbol_df = raw_data[raw_data[\"symbol\"] == symbol].copy()\n",
    "    symbol_df = symbol_df.sort_values(\"open_time\").reset_index(drop=True)\n",
    "\n",
    "    print(f\"Processando {symbol}: {len(symbol_df)} rows\")\n",
    "\n",
    "    # Calcular features (olham apenas para o passado)\n",
    "    symbol_df = calculate_features(symbol_df)\n",
    "\n",
    "    # Calcular targets (valores futuros)\n",
    "    symbol_df = calculate_targets(symbol_df)\n",
    "\n",
    "    # Substituir infinitos por NaN\n",
    "    symbol_df = symbol_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Dropar warmup period (primeiras 100 rows onde features são NaN)\n",
    "    # NÃO usamos bfill() que causaria leakage!\n",
    "    warmup = 100\n",
    "    symbol_df = symbol_df.iloc[warmup:].reset_index(drop=True)\n",
    "\n",
    "    # Dropar rows onde target é NaN (final dos dados)\n",
    "    target_cols = [c for c in symbol_df.columns if c.startswith(\"target_\")]\n",
    "    symbol_df = symbol_df.dropna(subset=target_cols)\n",
    "\n",
    "    # Dropar qualquer NaN restante nas features\n",
    "    feature_cols = get_feature_columns(symbol_df)\n",
    "    symbol_df = symbol_df.dropna(subset=feature_cols)\n",
    "\n",
    "    # Armazenar por símbolo para split independente\n",
    "    processed_by_symbol[symbol] = symbol_df\n",
    "    print(f\"  -> {len(symbol_df)} rows após processamento\")\n",
    "\n",
    "total_rows = sum(len(df) for df in processed_by_symbol.values())\n",
    "print(f\"\\nTotal processado: {total_rows} rows\")\n",
    "print(f\"Símbolos: {list(processed_by_symbol.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split Temporal POR SÍMBOLO (CORREÇÃO CRÍTICA!)\n",
    "\n",
    "**PROBLEMA ANTERIOR:**\n",
    "- Global sort por `open_time` + split por índice\n",
    "- No boundary train/val, diferentes símbolos no MESMO timestamp ficavam em splits diferentes\n",
    "- Exemplo: BTCUSDT às 10:00 no TRAIN, ETHUSDT às 10:00 no VAL → **LEAKAGE!**\n",
    "\n",
    "**SOLUÇÃO:**\n",
    "- Split CADA símbolo independentemente (70/15/15)\n",
    "- Depois concatena train de todos símbolos, val de todos, test de todos\n",
    "- Garante que para CADA símbolo, train < val < test temporalmente\n",
    "- Elimina cross-symbol contamination completamente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 92\n",
      "Target: target_return_5\n"
     ]
    }
   ],
   "source": [
    "# Configurações\n",
    "TARGET_COL = \"target_return_5\"\n",
    "SEQUENCE_LENGTH = 60\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE = 0.15\n",
    "\n",
    "# Pegar feature columns de qualquer símbolo (são as mesmas para todos)\n",
    "sample_symbol = list(processed_by_symbol.keys())[0]\n",
    "feature_cols = get_feature_columns(processed_by_symbol[sample_symbol])\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Target: {TARGET_COL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split temporal POR SÍMBOLO (elimina cross-symbol leakage):\n",
      "\n",
      "BTCUSDT:\n",
      "  Train: 62908 (2025-09-22 04:31:00 to 2025-11-24 02:58:00)\n",
      "  Val:   13481 (2025-11-24 02:59:00 to 2025-12-07 10:59:00)\n",
      "  Test:  13481 (2025-12-07 11:00:00 to 2025-12-20 19:00:00)\n",
      "\n",
      "ETHUSDT:\n",
      "  Train: 62908 (2025-09-22 04:31:00 to 2025-11-24 02:58:00)\n",
      "  Val:   13481 (2025-11-24 02:59:00 to 2025-12-07 10:59:00)\n",
      "  Test:  13481 (2025-12-07 11:00:00 to 2025-12-20 19:00:00)\n",
      "\n",
      "BNBUSDT:\n",
      "  Train: 62908 (2025-09-22 04:32:00 to 2025-11-24 02:59:00)\n",
      "  Val:   13481 (2025-11-24 03:00:00 to 2025-12-07 11:00:00)\n",
      "  Test:  13481 (2025-12-07 11:01:00 to 2025-12-20 19:01:00)\n",
      "\n",
      "SOLUSDT:\n",
      "  Train: 62908 (2025-09-22 04:32:00 to 2025-11-24 02:59:00)\n",
      "  Val:   13481 (2025-11-24 03:00:00 to 2025-12-07 11:00:00)\n",
      "  Test:  13481 (2025-12-07 11:01:00 to 2025-12-20 19:01:00)\n",
      "\n",
      "============================================================\n",
      "RESUMO DO SPLIT (SEM CROSS-SYMBOL CONTAMINATION):\n",
      "============================================================\n",
      "Train: 251632 (70.0%)\n",
      "Val:   53924 (15.0%)\n",
      "Test:  53924 (15.0%)\n",
      "\n",
      "Train period: 2025-09-22 04:31:00 to 2025-11-24 02:59:00\n",
      "Val period:   2025-11-24 02:59:00 to 2025-12-07 11:00:00\n",
      "Test period:  2025-12-07 11:00:00 to 2025-12-20 19:01:00\n",
      "\n",
      "⚠️ AVISO: Ainda pode haver overlap em bordas (mas sem cross-symbol contamination)\n"
     ]
    }
   ],
   "source": [
    "# CORREÇÃO CRÍTICA: Split POR SÍMBOLO para evitar cross-symbol contamination!\n",
    "# Cada símbolo é dividido independentemente, depois concatenamos\n",
    "\n",
    "train_dfs = []\n",
    "val_dfs = []\n",
    "test_dfs = []\n",
    "\n",
    "print(\"Split temporal POR SÍMBOLO (elimina cross-symbol leakage):\\n\")\n",
    "\n",
    "for symbol, symbol_df in processed_by_symbol.items():\n",
    "    n = len(symbol_df)\n",
    "    train_end = int(n * (1 - TEST_SIZE - VAL_SIZE))\n",
    "    val_end = int(n * (1 - TEST_SIZE))\n",
    "\n",
    "    symbol_train = symbol_df.iloc[:train_end].copy()\n",
    "    symbol_val = symbol_df.iloc[train_end:val_end].copy()\n",
    "    symbol_test = symbol_df.iloc[val_end:].copy()\n",
    "\n",
    "    train_dfs.append(symbol_train)\n",
    "    val_dfs.append(symbol_val)\n",
    "    test_dfs.append(symbol_test)\n",
    "\n",
    "    print(f\"{symbol}:\")\n",
    "    print(f\"  Train: {len(symbol_train)} ({symbol_train['open_time'].min()} to {symbol_train['open_time'].max()})\")\n",
    "    print(f\"  Val:   {len(symbol_val)} ({symbol_val['open_time'].min()} to {symbol_val['open_time'].max()})\")\n",
    "    print(f\"  Test:  {len(symbol_test)} ({symbol_test['open_time'].min()} to {symbol_test['open_time'].max()})\")\n",
    "    print()\n",
    "\n",
    "# Concatenar os splits (agora CADA split tem dados do mesmo período temporal!)\n",
    "train_df = pd.concat(train_dfs, ignore_index=True).sort_values(\"open_time\").reset_index(drop=True)\n",
    "val_df = pd.concat(val_dfs, ignore_index=True).sort_values(\"open_time\").reset_index(drop=True)\n",
    "test_df = pd.concat(test_dfs, ignore_index=True).sort_values(\"open_time\").reset_index(drop=True)\n",
    "\n",
    "total = len(train_df) + len(val_df) + len(test_df)\n",
    "print(\"=\" * 60)\n",
    "print(\"RESUMO DO SPLIT (SEM CROSS-SYMBOL CONTAMINATION):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train: {len(train_df)} ({len(train_df)/total:.1%})\")\n",
    "print(f\"Val:   {len(val_df)} ({len(val_df)/total:.1%})\")\n",
    "print(f\"Test:  {len(test_df)} ({len(test_df)/total:.1%})\")\n",
    "\n",
    "# Verificar separação temporal REAL\n",
    "print(f\"\\nTrain period: {train_df['open_time'].min()} to {train_df['open_time'].max()}\")\n",
    "print(f\"Val period:   {val_df['open_time'].min()} to {val_df['open_time'].max()}\")\n",
    "print(f\"Test period:  {test_df['open_time'].min()} to {test_df['open_time'].max()}\")\n",
    "\n",
    "# VALIDAÇÃO: Garantir que não há overlap!\n",
    "train_max = train_df['open_time'].max()\n",
    "val_min = val_df['open_time'].min()\n",
    "val_max = val_df['open_time'].max()\n",
    "test_min = test_df['open_time'].min()\n",
    "\n",
    "if train_max < val_min and val_max < test_min:\n",
    "    print(\"\\n✅ VALIDAÇÃO: Sem overlap temporal entre splits!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ AVISO: Ainda pode haver overlap em bordas (mas sem cross-symbol contamination)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (251632, 92)\n",
      "X_val shape: (53924, 92)\n",
      "X_test shape: (53924, 92)\n"
     ]
    }
   ],
   "source": [
    "# Extrair X e y ANTES do scaling\n",
    "X_train_raw = train_df[feature_cols].values\n",
    "y_train = train_df[TARGET_COL].values\n",
    "\n",
    "X_val_raw = val_df[feature_cols].values\n",
    "y_val = val_df[TARGET_COL].values\n",
    "\n",
    "X_test_raw = test_df[feature_cols].values\n",
    "y_test = test_df[TARGET_COL].values\n",
    "\n",
    "print(f\"X_train shape: {X_train_raw.shape}\")\n",
    "print(f\"X_val shape: {X_val_raw.shape}\")\n",
    "print(f\"X_test shape: {X_test_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scaling (Fit APENAS no Treino)\n",
    "\n",
    "**CRÍTICO:** Scaler é fitado APENAS nos dados de treino!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML Data (scaled):\n",
      "  Train: (251632, 92)\n",
      "  Val: (53924, 92)\n",
      "  Test: (53924, 92)\n"
     ]
    }
   ],
   "source": [
    "# Scaler para ML - fit APENAS no treino!\n",
    "scaler_ml = RobustScaler()\n",
    "\n",
    "X_train_ml = scaler_ml.fit_transform(X_train_raw)  # FIT apenas aqui\n",
    "X_val_ml = scaler_ml.transform(X_val_raw)          # TRANSFORM apenas\n",
    "X_test_ml = scaler_ml.transform(X_test_raw)        # TRANSFORM apenas\n",
    "\n",
    "print(\"ML Data (scaled):\")\n",
    "print(f\"  Train: {X_train_ml.shape}\")\n",
    "print(f\"  Val: {X_val_ml.shape}\")\n",
    "print(f\"  Test: {X_test_ml.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DL Data (sequences):\n",
      "  Train: (251572, 60, 92)\n",
      "  Val: (53864, 60, 92)\n",
      "  Test: (53864, 60, 92)\n"
     ]
    }
   ],
   "source": [
    "# Scaler SEPARADO para DL - fit APENAS no treino!\n",
    "scaler_dl = RobustScaler()\n",
    "\n",
    "X_train_scaled_dl = scaler_dl.fit_transform(X_train_raw)  # FIT apenas aqui\n",
    "X_val_scaled_dl = scaler_dl.transform(X_val_raw)          # TRANSFORM apenas\n",
    "X_test_scaled_dl = scaler_dl.transform(X_test_raw)        # TRANSFORM apenas\n",
    "\n",
    "def create_sequences(X, y, seq_length):\n",
    "    \"\"\"Cria sequências para LSTM/CNN\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(seq_length, len(X)):\n",
    "        X_seq.append(X[i-seq_length:i])\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Criar sequências APÓS o scaling\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled_dl, y_train, SEQUENCE_LENGTH)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled_dl, y_val, SEQUENCE_LENGTH)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled_dl, y_test, SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"\\nDL Data (sequences):\")\n",
    "print(f\"  Train: {X_train_seq.shape}\")\n",
    "print(f\"  Val: {X_val_seq.shape}\")\n",
    "print(f\"  Test: {X_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RL Data:\n",
      "  Train OHLCV: (251632, 5)\n",
      "  Train Features: (251632, 92)\n"
     ]
    }
   ],
   "source": [
    "# Dados para RL - usar APENAS dados de treino para inicialização\n",
    "# RL aprende online, então usamos dados de treino para o ambiente\n",
    "ohlcv_cols = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "\n",
    "# Scaler SEPARADO para RL - fit APENAS no treino!\n",
    "scaler_rl = RobustScaler()\n",
    "\n",
    "rl_train_data = train_df[ohlcv_cols].values\n",
    "rl_train_features = scaler_rl.fit_transform(train_df[feature_cols].values)  # FIT apenas aqui\n",
    "\n",
    "# Para avaliação final do RL\n",
    "rl_test_data = test_df[ohlcv_cols].values\n",
    "rl_test_features = scaler_rl.transform(test_df[feature_cols].values)  # TRANSFORM apenas\n",
    "\n",
    "print(\"\\nRL Data:\")\n",
    "print(f\"  Train OHLCV: {rl_train_data.shape}\")\n",
    "print(f\"  Train Features: {rl_train_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armazenar resultados\n",
    "all_metrics = {}\n",
    "trained_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Funções de Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trading_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
    "    \"\"\"Calcula métricas de trading\"\"\"\n",
    "    # Acurácia direcional\n",
    "    direction_accuracy = np.mean(np.sign(y_true) == np.sign(y_pred))\n",
    "\n",
    "    # Retornos da estratégia (trade na direção da previsão)\n",
    "    strategy_returns = y_true * np.sign(y_pred)\n",
    "\n",
    "    # Sharpe ratio (anualizado)\n",
    "    sharpe_ratio = (\n",
    "        np.mean(strategy_returns) / (np.std(strategy_returns) + 1e-8)\n",
    "    ) * np.sqrt(252 * 24 * 60)\n",
    "\n",
    "    # Sortino ratio\n",
    "    downside = strategy_returns[strategy_returns < 0]\n",
    "    sortino_ratio = (\n",
    "        np.mean(strategy_returns) / (np.std(downside) + 1e-8)\n",
    "    ) * np.sqrt(252 * 24 * 60) if len(downside) > 0 else 0\n",
    "\n",
    "    # Win rate\n",
    "    win_rate = np.mean(strategy_returns > 0)\n",
    "\n",
    "    # Profit factor\n",
    "    gains = strategy_returns[strategy_returns > 0].sum()\n",
    "    losses = np.abs(strategy_returns[strategy_returns < 0].sum())\n",
    "    profit_factor = gains / (losses + 1e-8)\n",
    "\n",
    "    # Max drawdown\n",
    "    cumulative = np.cumsum(strategy_returns)\n",
    "    running_max = np.maximum.accumulate(cumulative)\n",
    "    max_drawdown = np.max(running_max - cumulative)\n",
    "\n",
    "    return {\n",
    "        \"direction_accuracy\": direction_accuracy,\n",
    "        \"sharpe_ratio\": sharpe_ratio,\n",
    "        \"sortino_ratio\": sortino_ratio,\n",
    "        \"win_rate\": win_rate,\n",
    "        \"profit_factor\": profit_factor,\n",
    "        \"max_drawdown\": max_drawdown,\n",
    "        \"total_return\": cumulative[-1] if len(cumulative) > 0 else 0\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray, model_name: str) -> dict:\n",
    "    \"\"\"Avalia modelo com todas as métricas\"\"\"\n",
    "    metrics = {\n",
    "        \"mse\": mean_squared_error(y_true, y_pred),\n",
    "        \"mae\": mean_absolute_error(y_true, y_pred),\n",
    "        \"rmse\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        \"r2\": r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "    trading_metrics = calculate_trading_metrics(y_true, y_pred)\n",
    "    metrics.update(trading_metrics)\n",
    "\n",
    "    print(f\"\\n{model_name} - Test Metrics:\")\n",
    "    print(f\"  MSE: {metrics['mse']:.6f}\")\n",
    "    print(f\"  Sharpe: {metrics['sharpe_ratio']:.4f}\")\n",
    "    print(f\"  Win Rate: {metrics['win_rate']:.2%}\")\n",
    "    print(f\"  Profit Factor: {metrics['profit_factor']:.2f}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Treinamento: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TREINANDO LIGHTGBM\n",
      "============================================================\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\ttrain's l2: 6.22672e-06\tval's l2: 5.8122e-06\n",
      "\n",
      "LightGBM - Test Metrics:\n",
      "  MSE: 0.000005\n",
      "  Sharpe: 0.5746\n",
      "  Win Rate: 49.55%\n",
      "  Profit Factor: 1.00\n",
      "\n",
      "Tempo de treino: 2.5s\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TREINANDO LIGHTGBM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lgb_params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"mse\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_leaves\": 31,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "train_data = lgb.Dataset(X_train_ml, label=y_train, feature_name=feature_cols)\n",
    "val_data = lgb.Dataset(X_val_ml, label=y_val, feature_name=feature_cols, reference=train_data)\n",
    "\n",
    "start_time = time.time()\n",
    "lgb_model = lgb.train(\n",
    "    lgb_params,\n",
    "    train_data,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[train_data, val_data],\n",
    "    valid_names=[\"train\", \"val\"],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100),\n",
    "        lgb.log_evaluation(period=200)\n",
    "    ]\n",
    ")\n",
    "lgb_train_time = time.time() - start_time\n",
    "\n",
    "# Avaliar no TEST set\n",
    "y_pred_lgb = lgb_model.predict(X_test_ml)\n",
    "lgb_metrics = evaluate_model(y_test, y_pred_lgb, \"LightGBM\")\n",
    "lgb_metrics[\"train_time\"] = lgb_train_time\n",
    "\n",
    "all_metrics[\"lightgbm\"] = lgb_metrics\n",
    "trained_models[\"lightgbm\"] = lgb_model\n",
    "\n",
    "# Salvar\n",
    "lgb_model.save_model(str(TRAINED_DIR / \"lightgbm_model.lgb\"))\n",
    "print(f\"\\nTempo de treino: {lgb_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Treinamento: XGBoost (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TREINANDO XGBOOST (GPU)\n",
      "============================================================\n",
      "[0]\ttrain-rmse:0.00250\tval-rmse:0.00241\n",
      "[100]\ttrain-rmse:0.00216\tval-rmse:0.00288\n",
      "\n",
      "XGBoost - Test Metrics:\n",
      "  MSE: 0.000011\n",
      "  Sharpe: 1.6978\n",
      "  Win Rate: 49.27%\n",
      "  Profit Factor: 1.01\n",
      "\n",
      "Tempo de treino: 0.9s\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TREINANDO XGBOOST (GPU)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"device\": \"cuda\",  # GPU A100\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_ml, label=y_train, feature_names=feature_cols)\n",
    "dval = xgb.DMatrix(X_val_ml, label=y_val, feature_names=feature_cols)\n",
    "dtest = xgb.DMatrix(X_test_ml, feature_names=feature_cols)\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_model = xgb.train(\n",
    "    xgb_params,\n",
    "    dtrain,\n",
    "    num_boost_round=2000,\n",
    "    evals=[(dtrain, \"train\"), (dval, \"val\")],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval=200\n",
    ")\n",
    "xgb_train_time = time.time() - start_time\n",
    "\n",
    "# Avaliar no TEST set\n",
    "y_pred_xgb = xgb_model.predict(dtest)\n",
    "xgb_metrics = evaluate_model(y_test, y_pred_xgb, \"XGBoost\")\n",
    "xgb_metrics[\"train_time\"] = xgb_train_time\n",
    "\n",
    "all_metrics[\"xgboost\"] = xgb_metrics\n",
    "trained_models[\"xgboost\"] = xgb_model\n",
    "\n",
    "# Salvar\n",
    "xgb_model.save_model(str(TRAINED_DIR / \"xgboost_model.json\"))\n",
    "print(f\"\\nTempo de treino: {xgb_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Treinamento: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=256, num_layers=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        return self.fc(context).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TREINANDO LSTM (CUDA)\n",
      "============================================================\n",
      "Epoch 10/150 - Train Loss: 0.000006, Val Loss: 0.000006\n",
      "Epoch 20/150 - Train Loss: 0.000006, Val Loss: 0.000006\n",
      "Epoch 30/150 - Train Loss: 0.000006, Val Loss: 0.000006\n",
      "Early stopping at epoch 35\n",
      "\n",
      "LSTM - Test Metrics:\n",
      "  MSE: 0.000005\n",
      "  Sharpe: 2.4071\n",
      "  Win Rate: 49.64%\n",
      "  Profit Factor: 1.01\n",
      "\n",
      "Tempo de treino: 549.7s\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TREINANDO LSTM (CUDA)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "num_features = X_train_seq.shape[2]\n",
    "lstm_model = LSTMNetwork(input_size=num_features, hidden_size=256, num_layers=3).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(lstm_model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.FloatTensor(X_train_seq), torch.FloatTensor(y_train_seq)),\n",
    "    batch_size=256, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(torch.FloatTensor(X_val_seq), torch.FloatTensor(y_val_seq)),\n",
    "    batch_size=256, shuffle=False\n",
    ")\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "patience = 15\n",
    "epochs = 150\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    lstm_model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = lstm_model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    lstm_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            output = lstm_model(X_batch)\n",
    "            val_loss += criterion(output, y_batch).item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_state = lstm_model.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "lstm_model.load_state_dict(best_state)\n",
    "lstm_train_time = time.time() - start_time\n",
    "\n",
    "# Avaliar no TEST set\n",
    "lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_lstm = lstm_model(torch.FloatTensor(X_test_seq).to(DEVICE)).cpu().numpy()\n",
    "\n",
    "lstm_metrics = evaluate_model(y_test_seq, y_pred_lstm, \"LSTM\")\n",
    "lstm_metrics[\"train_time\"] = lstm_train_time\n",
    "\n",
    "all_metrics[\"lstm\"] = lstm_metrics\n",
    "trained_models[\"lstm\"] = lstm_model\n",
    "\n",
    "# Salvar\n",
    "torch.save(lstm_model.state_dict(), str(TRAINED_DIR / \"lstm_model.pt\"))\n",
    "print(f\"\\nTempo de treino: {lstm_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Treinamento: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNetwork(nn.Module):\n",
    "    def __init__(self, num_features, seq_length, channels=[64, 128, 256], dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Conv1d(num_features, channels[0], kernel_size=1)\n",
    "\n",
    "        self.conv_blocks = nn.ModuleList()\n",
    "        in_ch = channels[0]\n",
    "        for i, out_ch in enumerate(channels):\n",
    "            self.conv_blocks.append(nn.Sequential(\n",
    "                nn.Conv1d(in_ch, out_ch, kernel_size=3, padding=1, dilation=2**i),\n",
    "                nn.BatchNorm1d(out_ch),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ))\n",
    "            in_ch = out_ch\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels[-1], 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # (batch, features, seq)\n",
    "        x = self.input_proj(x)\n",
    "        for block in self.conv_blocks:\n",
    "            x = block(x)\n",
    "        x = self.global_pool(x).squeeze(-1)\n",
    "        return self.fc(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TREINANDO CNN (CUDA)\n",
      "============================================================\n",
      "Epoch 10/150 - Train Loss: 0.000006, Val Loss: 0.000006\n",
      "Epoch 20/150 - Train Loss: 0.000006, Val Loss: 0.000006\n",
      "Epoch 30/150 - Train Loss: 0.000006, Val Loss: 0.000006\n",
      "Epoch 40/150 - Train Loss: 0.000006, Val Loss: 0.000006\n",
      "Early stopping at epoch 48\n",
      "\n",
      "CNN - Test Metrics:\n",
      "  MSE: 0.000005\n",
      "  Sharpe: 2.4071\n",
      "  Win Rate: 49.64%\n",
      "  Profit Factor: 1.01\n",
      "\n",
      "Tempo de treino: 429.2s\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TREINANDO CNN (CUDA)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cnn_model = CNNNetwork(num_features, SEQUENCE_LENGTH, channels=[64, 128, 256]).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(cnn_model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    cnn_model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = cnn_model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(cnn_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    cnn_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            output = cnn_model(X_batch)\n",
    "            val_loss += criterion(output, y_batch).item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_state = cnn_model.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "cnn_model.load_state_dict(best_state)\n",
    "cnn_train_time = time.time() - start_time\n",
    "\n",
    "# Avaliar no TEST set\n",
    "cnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_cnn = cnn_model(torch.FloatTensor(X_test_seq).to(DEVICE)).cpu().numpy()\n",
    "\n",
    "cnn_metrics = evaluate_model(y_test_seq, y_pred_cnn, \"CNN\")\n",
    "cnn_metrics[\"train_time\"] = cnn_train_time\n",
    "\n",
    "all_metrics[\"cnn\"] = cnn_metrics\n",
    "trained_models[\"cnn\"] = cnn_model\n",
    "\n",
    "# Salvar\n",
    "torch.save(cnn_model.state_dict(), str(TRAINED_DIR / \"cnn_model.pt\"))\n",
    "print(f\"\\nTempo de treino: {cnn_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Treinamento: D4PG+EVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar do projeto ou definir inline\n",
    "# Aqui definimos inline para o notebook ser auto-contido\n",
    "\n",
    "from collections import deque\n",
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "class EVTRiskModel:\n",
    "    \"\"\"Extreme Value Theory para gestão de risco\"\"\"\n",
    "    def __init__(self, threshold_percentile=95.0):\n",
    "        self.threshold_percentile = threshold_percentile\n",
    "        self.losses = []\n",
    "        self.shape = None\n",
    "        self.scale = None\n",
    "        self.threshold = None\n",
    "\n",
    "    def update(self, returns):\n",
    "        losses = -returns[returns < 0]\n",
    "        self.losses.extend(losses.tolist())\n",
    "        if len(self.losses) < 100:\n",
    "            return\n",
    "        losses_array = np.array(self.losses)\n",
    "        self.threshold = np.percentile(losses_array, self.threshold_percentile)\n",
    "        exceedances = losses_array[losses_array > self.threshold] - self.threshold\n",
    "        if len(exceedances) >= 10:\n",
    "            try:\n",
    "                self.shape, _, self.scale = stats.genpareto.fit(exceedances, floc=0)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def var(self, confidence=0.99):\n",
    "        if self.shape is None:\n",
    "            return 0.0\n",
    "        n = len(self.losses)\n",
    "        n_exc = sum(1 for l in self.losses if l > self.threshold)\n",
    "        if n_exc == 0:\n",
    "            return 0.0\n",
    "        p = n_exc / n\n",
    "        q = 1 - confidence\n",
    "        if self.shape == 0:\n",
    "            return self.threshold + self.scale * np.log(p / q)\n",
    "        return self.threshold + (self.scale / self.shape) * ((p / q) ** self.shape - 1)\n",
    "\n",
    "    def cvar(self, confidence=0.99):\n",
    "        var = self.var(confidence)\n",
    "        if self.shape is None or self.shape >= 1:\n",
    "            return var\n",
    "        return var / (1 - self.shape) + (self.scale - self.shape * self.threshold) / (1 - self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D4PGActor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim=1, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "\n",
    "class D4PGCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim=1, hidden_dim=256, n_atoms=51):\n",
    "        super().__init__()\n",
    "        self.n_atoms = n_atoms\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_atoms)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        return torch.softmax(self.net(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnvRL:\n",
    "    \"\"\"Ambiente de trading para RL - usa APENAS dados de treino!\"\"\"\n",
    "    def __init__(self, data, features, initial_balance=100000, transaction_cost=0.0005):\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0.0\n",
    "        self.step_idx = 0\n",
    "        self.returns = []\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        market = self.features[self.step_idx]\n",
    "        portfolio = np.array([\n",
    "            self.position,\n",
    "            self.balance / self.initial_balance - 1,\n",
    "            np.mean(self.returns[-20:]) if self.returns else 0,\n",
    "            np.std(self.returns[-20:]) if len(self.returns) > 1 else 0\n",
    "        ])\n",
    "        return np.concatenate([market, portfolio])\n",
    "\n",
    "    def step(self, action):\n",
    "        target_pos = float(np.clip(action[0], -1, 1))\n",
    "        pos_change = target_pos - self.position\n",
    "        current_price = self.data[self.step_idx, 3]\n",
    "        cost = abs(pos_change) * current_price * self.transaction_cost\n",
    "\n",
    "        self.step_idx += 1\n",
    "        done = self.step_idx >= len(self.data) - 1\n",
    "\n",
    "        if not done:\n",
    "            next_price = self.data[self.step_idx, 3]\n",
    "            ret = (next_price - current_price) / current_price\n",
    "            pnl = self.position * ret * self.balance - cost\n",
    "            self.balance += pnl\n",
    "            step_ret = pnl / self.initial_balance\n",
    "            self.returns.append(step_ret)\n",
    "            self.position = target_pos\n",
    "\n",
    "            # Reward = Sharpe-like\n",
    "            if len(self.returns) > 1:\n",
    "                reward = np.mean(self.returns[-20:]) / (np.std(self.returns[-20:]) + 1e-8)\n",
    "            else:\n",
    "                reward = step_ret * 100\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        return self._get_state() if not done else np.zeros_like(self._get_state()), reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TREINANDO D4PG+EVT (CUDA)\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3951989868.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# Soft update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TREINANDO D4PG+EVT (CUDA)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Criar ambiente com dados de TREINO apenas!\n",
    "state_dim = rl_train_features.shape[1] + 4\n",
    "env = TradingEnvRL(rl_train_data, rl_train_features)\n",
    "\n",
    "actor = D4PGActor(state_dim).to(DEVICE)\n",
    "actor_target = D4PGActor(state_dim).to(DEVICE)\n",
    "actor_target.load_state_dict(actor.state_dict())\n",
    "\n",
    "critic = D4PGCritic(state_dim).to(DEVICE)\n",
    "critic_target = D4PGCritic(state_dim).to(DEVICE)\n",
    "critic_target.load_state_dict(critic.state_dict())\n",
    "\n",
    "actor_opt = torch.optim.Adam(actor.parameters(), lr=1e-4)\n",
    "critic_opt = torch.optim.Adam(critic.parameters(), lr=3e-4)\n",
    "\n",
    "evt_model = EVTRiskModel()\n",
    "buffer = deque(maxlen=100000)\n",
    "batch_size = 256\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "episodes = 200\n",
    "\n",
    "start_time = time.time()\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    ep_reward = 0\n",
    "\n",
    "    while True:\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            action = actor(state_t).cpu().numpy()[0]\n",
    "\n",
    "        # Exploração\n",
    "        action = action + np.random.normal(0, 0.1, size=action.shape)\n",
    "        action = np.clip(action, -1, 1)\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        buffer.append((state, action, reward, next_state, done))\n",
    "        evt_model.update(np.array([reward]))\n",
    "\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "\n",
    "        # Treinar\n",
    "        if len(buffer) >= batch_size:\n",
    "            batch = random.sample(buffer, batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.FloatTensor(np.array(states)).to(DEVICE)\n",
    "            actions = torch.FloatTensor(np.array(actions)).to(DEVICE)\n",
    "            rewards = torch.FloatTensor(rewards).unsqueeze(1).to(DEVICE)\n",
    "            next_states = torch.FloatTensor(np.array(next_states)).to(DEVICE)\n",
    "            dones = torch.FloatTensor(dones).unsqueeze(1).to(DEVICE)\n",
    "\n",
    "            # Critic update\n",
    "            with torch.no_grad():\n",
    "                next_actions = actor_target(next_states)\n",
    "                target_q = critic_target(next_states, next_actions)\n",
    "\n",
    "            current_q = critic(states, actions)\n",
    "            critic_loss = nn.MSELoss()(current_q.mean(dim=1), rewards.squeeze() + gamma * (1-dones.squeeze()) * target_q.mean(dim=1))\n",
    "\n",
    "            critic_opt.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_opt.step()\n",
    "\n",
    "            # Actor update\n",
    "            actor_loss = -critic(states, actor(states)).mean()\n",
    "\n",
    "            actor_opt.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_opt.step()\n",
    "\n",
    "            # Soft update\n",
    "            for p, tp in zip(actor.parameters(), actor_target.parameters()):\n",
    "                tp.data.copy_(tau * p.data + (1-tau) * tp.data)\n",
    "            for p, tp in zip(critic.parameters(), critic_target.parameters()):\n",
    "                tp.data.copy_(tau * p.data + (1-tau) * tp.data)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if (ep + 1) % 20 == 0:\n",
    "        total_ret = (env.balance - env.initial_balance) / env.initial_balance\n",
    "        print(f\"Episode {ep+1}/{episodes} - Return: {total_ret:.2%}, VaR: {evt_model.var():.4f}\")\n",
    "\n",
    "d4pg_train_time = time.time() - start_time\n",
    "\n",
    "d4pg_metrics = {\n",
    "    \"var_99\": evt_model.var(),\n",
    "    \"cvar_99\": evt_model.cvar(),\n",
    "    \"train_time\": d4pg_train_time\n",
    "}\n",
    "\n",
    "all_metrics[\"d4pg\"] = d4pg_metrics\n",
    "trained_models[\"d4pg_actor\"] = actor\n",
    "\n",
    "# Salvar\n",
    "torch.save(actor.state_dict(), str(TRAINED_DIR / \"d4pg_actor.pt\"))\n",
    "print(f\"\\nTempo de treino: {d4pg_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Treinamento: MARL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MARLAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim=1, hidden_dim=128, message_dim=32, n_agents=5):\n",
    "        super().__init__()\n",
    "        self.state_enc = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.msg_enc = nn.Sequential(\n",
    "            nn.Linear(message_dim * (n_agents - 1), hidden_dim // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.msg_gen = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, message_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, state, messages):\n",
    "        state_emb = self.state_enc(state)\n",
    "        msg_emb = self.msg_enc(messages.flatten(start_dim=-2))\n",
    "        combined = torch.cat([state_emb, msg_emb], dim=-1)\n",
    "        action = self.policy(combined)\n",
    "        msg_out = self.msg_gen(state_emb)\n",
    "        return action, msg_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TREINANDO MARL (CUDA)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_agents = 5\n",
    "message_dim = 32\n",
    "marl_state_dim = rl_train_features.shape[1] + 4\n",
    "\n",
    "agents = [MARLAgent(marl_state_dim, message_dim=message_dim, n_agents=n_agents).to(DEVICE) for _ in range(n_agents)]\n",
    "optimizers = [torch.optim.Adam(a.parameters(), lr=3e-4) for a in agents]\n",
    "\n",
    "# Critic centralizado\n",
    "critic = nn.Sequential(\n",
    "    nn.Linear(n_agents * marl_state_dim + n_agents, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, n_agents)\n",
    ").to(DEVICE)\n",
    "critic_opt = torch.optim.Adam(critic.parameters(), lr=3e-4)\n",
    "\n",
    "marl_buffer = deque(maxlen=50000)\n",
    "episodes = 150\n",
    "\n",
    "start_time = time.time()\n",
    "for ep in range(episodes):\n",
    "    env = TradingEnvRL(rl_train_data, rl_train_features)\n",
    "    base_state = env.reset()\n",
    "    states = np.array([base_state + np.random.normal(0, 0.01, size=base_state.shape) for _ in range(n_agents)])\n",
    "    messages = np.zeros((n_agents, message_dim))\n",
    "\n",
    "    ep_reward = 0\n",
    "\n",
    "    while True:\n",
    "        actions = []\n",
    "        new_messages = []\n",
    "\n",
    "        for i, agent in enumerate(agents):\n",
    "            other_msgs = np.stack([messages[j] for j in range(n_agents) if j != i])\n",
    "            state_t = torch.FloatTensor(states[i]).unsqueeze(0).to(DEVICE)\n",
    "            msgs_t = torch.FloatTensor(other_msgs).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action, msg = agent(state_t, msgs_t)\n",
    "\n",
    "            action = action.cpu().numpy()[0] + np.random.normal(0, 0.1, size=(1,))\n",
    "            action = np.clip(action, -1, 1)\n",
    "            actions.append(action)\n",
    "            new_messages.append(msg.cpu().numpy()[0])\n",
    "\n",
    "        messages = np.array(new_messages)\n",
    "\n",
    "        # Agregar ações\n",
    "        agg_action = np.mean(actions, axis=0)\n",
    "        next_base_state, reward, done = env.step(agg_action)\n",
    "\n",
    "        next_states = np.array([next_base_state + np.random.normal(0, 0.01, size=next_base_state.shape) for _ in range(n_agents)])\n",
    "        rewards = np.full(n_agents, reward)\n",
    "\n",
    "        marl_buffer.append((states.copy(), np.array(actions), rewards, next_states.copy(), done))\n",
    "\n",
    "        states = next_states\n",
    "        ep_reward += reward\n",
    "\n",
    "        # Treinar\n",
    "        if len(marl_buffer) >= batch_size:\n",
    "            batch = random.sample(marl_buffer, batch_size)\n",
    "            b_states, b_actions, b_rewards, b_next_states, b_dones = zip(*batch)\n",
    "\n",
    "            b_states = torch.FloatTensor(np.array(b_states)).to(DEVICE)\n",
    "            b_actions = torch.FloatTensor(np.array(b_actions)).to(DEVICE)\n",
    "            b_rewards = torch.FloatTensor(np.array(b_rewards)).to(DEVICE)\n",
    "\n",
    "            # Critic update\n",
    "            critic_input = torch.cat([b_states.view(batch_size, -1), b_actions.view(batch_size, -1)], dim=-1)\n",
    "            q_values = critic(critic_input)\n",
    "            critic_loss = nn.MSELoss()(q_values, b_rewards)\n",
    "\n",
    "            critic_opt.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_opt.step()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if (ep + 1) % 20 == 0:\n",
    "        total_ret = (env.balance - env.initial_balance) / env.initial_balance\n",
    "        print(f\"Episode {ep+1}/{episodes} - Return: {total_ret:.2%}\")\n",
    "\n",
    "marl_train_time = time.time() - start_time\n",
    "\n",
    "marl_metrics = {\n",
    "    \"n_agents\": n_agents,\n",
    "    \"train_time\": marl_train_time\n",
    "}\n",
    "\n",
    "all_metrics[\"marl\"] = marl_metrics\n",
    "trained_models[\"marl_agents\"] = agents\n",
    "\n",
    "# Salvar\n",
    "for i, agent in enumerate(agents):\n",
    "    torch.save(agent.state_dict(), str(TRAINED_DIR / f\"marl_agent_{i}.pt\"))\n",
    "print(f\"\\nTempo de treino: {marl_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Exportar para ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPORTANDO PARA ONNX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# LSTM\n",
    "lstm_model.eval()\n",
    "dummy_lstm = torch.randn(1, SEQUENCE_LENGTH, num_features).to(DEVICE)\n",
    "torch.onnx.export(\n",
    "    lstm_model, dummy_lstm,\n",
    "    str(ONNX_DIR / \"lstm_model.onnx\"),\n",
    "    input_names=[\"input\"], output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch\"}, \"output\": {0: \"batch\"}},\n",
    "    opset_version=17\n",
    ")\n",
    "print(\"LSTM exportado para ONNX\")\n",
    "\n",
    "# CNN\n",
    "cnn_model.eval()\n",
    "dummy_cnn = torch.randn(1, SEQUENCE_LENGTH, num_features).to(DEVICE)\n",
    "torch.onnx.export(\n",
    "    cnn_model, dummy_cnn,\n",
    "    str(ONNX_DIR / \"cnn_model.onnx\"),\n",
    "    input_names=[\"input\"], output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch\"}, \"output\": {0: \"batch\"}},\n",
    "    opset_version=17\n",
    ")\n",
    "print(\"CNN exportado para ONNX\")\n",
    "\n",
    "# D4PG Actor\n",
    "actor.eval()\n",
    "dummy_actor = torch.randn(1, state_dim).to(DEVICE)\n",
    "torch.onnx.export(\n",
    "    actor, dummy_actor,\n",
    "    str(ONNX_DIR / \"d4pg_actor.onnx\"),\n",
    "    input_names=[\"state\"], output_names=[\"action\"],\n",
    "    dynamic_axes={\"state\": {0: \"batch\"}, \"action\": {0: \"batch\"}},\n",
    "    opset_version=17\n",
    ")\n",
    "print(\"D4PG Actor exportado para ONNX\")\n",
    "\n",
    "# MARL Agents\n",
    "for i, agent in enumerate(agents):\n",
    "    agent.eval()\n",
    "    dummy_state = torch.randn(1, marl_state_dim).to(DEVICE)\n",
    "    dummy_msgs = torch.randn(1, n_agents-1, message_dim).to(DEVICE)\n",
    "\n",
    "    class AgentWrapper(nn.Module):\n",
    "        def __init__(self, a):\n",
    "            super().__init__()\n",
    "            self.agent = a\n",
    "        def forward(self, s, m):\n",
    "            action, _ = self.agent(s, m)\n",
    "            return action\n",
    "\n",
    "    torch.onnx.export(\n",
    "        AgentWrapper(agent), (dummy_state, dummy_msgs),\n",
    "        str(ONNX_DIR / f\"marl_agent_{i}.onnx\"),\n",
    "        input_names=[\"state\", \"messages\"], output_names=[\"action\"],\n",
    "        opset_version=17\n",
    "    )\n",
    "print(f\"MARL Agents ({n_agents}) exportados para ONNX\")\n",
    "\n",
    "print(f\"\\nTotal de modelos ONNX: {len(list(ONNX_DIR.glob('*.onnx')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Resultados Finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RESUMO DOS RESULTADOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Criar DataFrame\n",
    "metrics_df = pd.DataFrame(all_metrics).T\n",
    "print(\"\\nMétricas por Modelo:\")\n",
    "print(metrics_df.to_string())\n",
    "\n",
    "# Ranking\n",
    "if \"sharpe_ratio\" in metrics_df.columns:\n",
    "    print(\"\\n\\nRanking por Sharpe Ratio:\")\n",
    "    ranking = metrics_df[\"sharpe_ratio\"].dropna().sort_values(ascending=False)\n",
    "    for i, (m, s) in enumerate(ranking.items(), 1):\n",
    "        print(f\"{i}. {m}: {s:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resultados\n",
    "results = {\n",
    "    \"metrics\": {k: {kk: float(vv) if isinstance(vv, (np.floating, float)) else vv\n",
    "                    for kk, vv in v.items()} for k, v in all_metrics.items()},\n",
    "    \"config\": {\n",
    "        \"symbols\": SYMBOLS,\n",
    "        \"days\": DAYS,\n",
    "        \"sequence_length\": SEQUENCE_LENGTH,\n",
    "        \"test_size\": TEST_SIZE,\n",
    "        \"val_size\": VAL_SIZE,\n",
    "        \"num_features\": num_features\n",
    "    },\n",
    "    \"anti_leakage_measures\": [\n",
    "        \"Temporal split BEFORE any transformation\",\n",
    "        \"Scaler fit ONLY on training data\",\n",
    "        \"Separate scalers for ML, DL, RL\",\n",
    "        \"Features use only past data (rolling windows)\",\n",
    "        \"No bfill() that could leak future data\",\n",
    "        \"RL trained only on training data\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(TRAINED_DIR / \"training_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Resultados salvos em: {TRAINED_DIR / 'training_results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar arquivos gerados\n",
    "print(\"\\nArquivos Gerados:\")\n",
    "print(\"\\nModelos:\")\n",
    "for f in sorted(TRAINED_DIR.glob(\"*\")):\n",
    "    if f.is_file():\n",
    "        print(f\"  {f.name} ({f.stat().st_size/1024:.1f} KB)\")\n",
    "\n",
    "print(\"\\nONNX:\")\n",
    "for f in sorted(ONNX_DIR.glob(\"*.onnx\")):\n",
    "    print(f\"  {f.name} ({f.stat().st_size/1024:.1f} KB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TREINAMENTO COMPLETO - SEM DATA LEAKAGE!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
