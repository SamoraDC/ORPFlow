# XGBoost Training Configuration
# ================================
# Optimized for BTCUSDT 1-minute bars trading

# Data settings
data:
  data_path: "data/processed/features.parquet"
  target_column: "target_return_5"
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15

# XGBoost hyperparameters (optimized via Optuna)
model:
  n_estimators: 1000
  max_depth: 6
  learning_rate: 0.01
  subsample: 0.8
  colsample_bytree: 0.8
  min_child_weight: 3
  gamma: 0.1
  reg_alpha: 0.1
  reg_lambda: 1.0

  # Tree method
  tree_method: "hist"  # fast histogram-based algorithm

  # Early stopping
  early_stopping_rounds: 50

  # Objective
  objective: "reg:squarederror"
  eval_metric: "rmse"

  # Random seed
  random_state: 42

# CPCV (Combinatorial Purged Cross-Validation)
cpcv:
  enabled: true
  n_splits: 5
  n_test_groups: 2
  embargo_pct: 0.01  # 1% embargo gap
  purge_pct: 0.005   # 0.5% purge gap

# Optuna hyperparameter optimization
optuna:
  enabled: false
  n_trials: 100
  timeout: 7200  # 2 hours

  # Search space
  search_space:
    n_estimators: [500, 2000]
    max_depth: [4, 10]
    learning_rate: [0.001, 0.1]
    subsample: [0.6, 1.0]
    colsample_bytree: [0.6, 1.0]
    min_child_weight: [1, 10]
    gamma: [0.0, 0.5]
    reg_alpha: [0.0, 1.0]
    reg_lambda: [0.5, 2.0]

# Feature selection
features:
  importance_threshold: 0.001  # Remove features with <0.1% importance
  max_features: null           # Use all features if null

# Output settings
output:
  output_dir: "trained"
  model_name: "xgboost_model"
  export_onnx: true
  save_feature_importance: true

# Logging
logging:
  level: "INFO"
  log_to_file: true
  log_file: "logs/xgboost_training.log"
